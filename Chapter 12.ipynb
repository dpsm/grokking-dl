{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks that Write like Shakesphere\n",
    "## Recurrent Layers for Variable Length Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>type</th>\n",
       "      <th>review</th>\n",
       "      <th>label</th>\n",
       "      <th>file</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>test</td>\n",
       "      <td>Once again Mr. Costner has dragged out a movie...</td>\n",
       "      <td>neg</td>\n",
       "      <td>0_2.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>test</td>\n",
       "      <td>This is an example of why the majority of acti...</td>\n",
       "      <td>neg</td>\n",
       "      <td>10000_4.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>test</td>\n",
       "      <td>First of all I hate those moronic rappers, who...</td>\n",
       "      <td>neg</td>\n",
       "      <td>10001_1.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>test</td>\n",
       "      <td>Not even the Beatles could write songs everyon...</td>\n",
       "      <td>neg</td>\n",
       "      <td>10002_3.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>test</td>\n",
       "      <td>Brass pictures (movies is not a fitting word f...</td>\n",
       "      <td>neg</td>\n",
       "      <td>10003_3.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>test</td>\n",
       "      <td>A funny thing happened to me while watching \"M...</td>\n",
       "      <td>neg</td>\n",
       "      <td>10004_2.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>test</td>\n",
       "      <td>This German horror film has to be one of the w...</td>\n",
       "      <td>neg</td>\n",
       "      <td>10005_2.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>test</td>\n",
       "      <td>Being a long-time fan of Japanese film, I expe...</td>\n",
       "      <td>neg</td>\n",
       "      <td>10006_2.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>test</td>\n",
       "      <td>\"Tokyo Eyes\" tells of a 17 year old Japanese g...</td>\n",
       "      <td>neg</td>\n",
       "      <td>10007_4.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>test</td>\n",
       "      <td>Wealthy horse ranchers in Buenos Aires have a ...</td>\n",
       "      <td>neg</td>\n",
       "      <td>10008_4.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>test</td>\n",
       "      <td>Cage plays a drunk and gets high critically pr...</td>\n",
       "      <td>neg</td>\n",
       "      <td>10009_3.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>test</td>\n",
       "      <td>First of all, I would like to say that I am a ...</td>\n",
       "      <td>neg</td>\n",
       "      <td>1000_3.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "      <td>test</td>\n",
       "      <td>So tell me - what serious boozer drinks Budwei...</td>\n",
       "      <td>neg</td>\n",
       "      <td>10010_2.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13</td>\n",
       "      <td>test</td>\n",
       "      <td>A big disappointment for what was touted as an...</td>\n",
       "      <td>neg</td>\n",
       "      <td>10011_1.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14</td>\n",
       "      <td>test</td>\n",
       "      <td>This film is absolutely appalling and awful. I...</td>\n",
       "      <td>neg</td>\n",
       "      <td>10012_1.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>15</td>\n",
       "      <td>test</td>\n",
       "      <td>Here's a decidedly average Italian post apocal...</td>\n",
       "      <td>neg</td>\n",
       "      <td>10013_4.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>16</td>\n",
       "      <td>test</td>\n",
       "      <td>At the bottom end of the apocalypse movie scal...</td>\n",
       "      <td>neg</td>\n",
       "      <td>10014_2.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>17</td>\n",
       "      <td>test</td>\n",
       "      <td>Earth has been destroyed in a nuclear holocaus...</td>\n",
       "      <td>neg</td>\n",
       "      <td>10015_4.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>18</td>\n",
       "      <td>test</td>\n",
       "      <td>Many people are standing in front of the house...</td>\n",
       "      <td>neg</td>\n",
       "      <td>10016_3.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>19</td>\n",
       "      <td>test</td>\n",
       "      <td>New York family is the last in their neighborh...</td>\n",
       "      <td>neg</td>\n",
       "      <td>10017_1.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>20</td>\n",
       "      <td>test</td>\n",
       "      <td>The best thing about \"The Prey\" is the tag lin...</td>\n",
       "      <td>neg</td>\n",
       "      <td>10018_1.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>21</td>\n",
       "      <td>test</td>\n",
       "      <td>This is truly, without exaggerating, one of th...</td>\n",
       "      <td>neg</td>\n",
       "      <td>10019_1.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>22</td>\n",
       "      <td>test</td>\n",
       "      <td>I'm a huge fan of both Emily Watson (Breaking ...</td>\n",
       "      <td>neg</td>\n",
       "      <td>1001_4.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>23</td>\n",
       "      <td>test</td>\n",
       "      <td>Sure, most of the slasher films of the 1980's ...</td>\n",
       "      <td>neg</td>\n",
       "      <td>10020_1.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>24</td>\n",
       "      <td>test</td>\n",
       "      <td>I think that would have been a more appropriat...</td>\n",
       "      <td>neg</td>\n",
       "      <td>10021_3.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>25</td>\n",
       "      <td>test</td>\n",
       "      <td>1980 was certainly a year for bad backwoods sl...</td>\n",
       "      <td>neg</td>\n",
       "      <td>10022_4.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>26</td>\n",
       "      <td>test</td>\n",
       "      <td>Everything everyone has said already pretty mu...</td>\n",
       "      <td>neg</td>\n",
       "      <td>10023_4.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>27</td>\n",
       "      <td>test</td>\n",
       "      <td>Uhhh ... so, did they even have writers for th...</td>\n",
       "      <td>neg</td>\n",
       "      <td>10024_3.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>28</td>\n",
       "      <td>test</td>\n",
       "      <td>Oh yeah, this one is definitely a strong conte...</td>\n",
       "      <td>neg</td>\n",
       "      <td>10025_2.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>29</td>\n",
       "      <td>test</td>\n",
       "      <td>Supercraptastic slasher fare, which feels over...</td>\n",
       "      <td>neg</td>\n",
       "      <td>10026_3.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99970</th>\n",
       "      <td>99970</td>\n",
       "      <td>train</td>\n",
       "      <td>This is without a doubt the worst movie I have...</td>\n",
       "      <td>unsup</td>\n",
       "      <td>9975_0.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99971</th>\n",
       "      <td>99971</td>\n",
       "      <td>train</td>\n",
       "      <td>Note: I've only seen the MST version of this f...</td>\n",
       "      <td>unsup</td>\n",
       "      <td>9976_0.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99972</th>\n",
       "      <td>99972</td>\n",
       "      <td>train</td>\n",
       "      <td>I've seen the MST3K version of this movie unde...</td>\n",
       "      <td>unsup</td>\n",
       "      <td>9977_0.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99973</th>\n",
       "      <td>99973</td>\n",
       "      <td>train</td>\n",
       "      <td>This disillusioning tale of time travel is mor...</td>\n",
       "      <td>unsup</td>\n",
       "      <td>9978_0.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99974</th>\n",
       "      <td>99974</td>\n",
       "      <td>train</td>\n",
       "      <td>All right, enough with the MST3K-ing, people. ...</td>\n",
       "      <td>unsup</td>\n",
       "      <td>9979_0.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99975</th>\n",
       "      <td>99975</td>\n",
       "      <td>train</td>\n",
       "      <td>I saw this in Pittsburgh PA in a small theater...</td>\n",
       "      <td>unsup</td>\n",
       "      <td>997_0.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99976</th>\n",
       "      <td>99976</td>\n",
       "      <td>train</td>\n",
       "      <td>And then filmed during each actors day off at ...</td>\n",
       "      <td>unsup</td>\n",
       "      <td>9980_0.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99977</th>\n",
       "      <td>99977</td>\n",
       "      <td>train</td>\n",
       "      <td>Hey, it's not a time machine, it's a \"time tra...</td>\n",
       "      <td>unsup</td>\n",
       "      <td>9981_0.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99978</th>\n",
       "      <td>99978</td>\n",
       "      <td>train</td>\n",
       "      <td>This backyard movie has it all: time travel, a...</td>\n",
       "      <td>unsup</td>\n",
       "      <td>9982_0.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99979</th>\n",
       "      <td>99979</td>\n",
       "      <td>train</td>\n",
       "      <td>It was a true labor to sit through this \"movie...</td>\n",
       "      <td>unsup</td>\n",
       "      <td>9983_0.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99980</th>\n",
       "      <td>99980</td>\n",
       "      <td>train</td>\n",
       "      <td>The comments already posted for this film do n...</td>\n",
       "      <td>unsup</td>\n",
       "      <td>9984_0.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99981</th>\n",
       "      <td>99981</td>\n",
       "      <td>train</td>\n",
       "      <td>One of the funnier MST3K experiments. This was...</td>\n",
       "      <td>unsup</td>\n",
       "      <td>9985_0.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99982</th>\n",
       "      <td>99982</td>\n",
       "      <td>train</td>\n",
       "      <td>Time Chasers is probably one of the better mov...</td>\n",
       "      <td>unsup</td>\n",
       "      <td>9986_0.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99983</th>\n",
       "      <td>99983</td>\n",
       "      <td>train</td>\n",
       "      <td>... to dull the incredible pain this stinkmast...</td>\n",
       "      <td>unsup</td>\n",
       "      <td>9987_0.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99984</th>\n",
       "      <td>99984</td>\n",
       "      <td>train</td>\n",
       "      <td>I feel badly for the actors and directors of t...</td>\n",
       "      <td>unsup</td>\n",
       "      <td>9988_0.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99985</th>\n",
       "      <td>99985</td>\n",
       "      <td>train</td>\n",
       "      <td>I'm not really sure they were trying to make a...</td>\n",
       "      <td>unsup</td>\n",
       "      <td>9989_0.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99986</th>\n",
       "      <td>99986</td>\n",
       "      <td>train</td>\n",
       "      <td>I got a screener of this film a couple of week...</td>\n",
       "      <td>unsup</td>\n",
       "      <td>998_0.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99987</th>\n",
       "      <td>99987</td>\n",
       "      <td>train</td>\n",
       "      <td>****SPOILERS****SPOILERS****SPOILERS****SPOILE...</td>\n",
       "      <td>unsup</td>\n",
       "      <td>9990_0.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99988</th>\n",
       "      <td>99988</td>\n",
       "      <td>train</td>\n",
       "      <td>This is a really dumb movie, with a very low b...</td>\n",
       "      <td>unsup</td>\n",
       "      <td>9991_0.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99989</th>\n",
       "      <td>99989</td>\n",
       "      <td>train</td>\n",
       "      <td>Nick Miller has developed a means of time trav...</td>\n",
       "      <td>unsup</td>\n",
       "      <td>9992_0.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99990</th>\n",
       "      <td>99990</td>\n",
       "      <td>train</td>\n",
       "      <td>That's what the director of this film had to b...</td>\n",
       "      <td>unsup</td>\n",
       "      <td>9993_0.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99991</th>\n",
       "      <td>99991</td>\n",
       "      <td>train</td>\n",
       "      <td>Think of this one as Back to the Future...exce...</td>\n",
       "      <td>unsup</td>\n",
       "      <td>9994_0.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99992</th>\n",
       "      <td>99992</td>\n",
       "      <td>train</td>\n",
       "      <td>What happened to the 'second' hero who went ba...</td>\n",
       "      <td>unsup</td>\n",
       "      <td>9995_0.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99993</th>\n",
       "      <td>99993</td>\n",
       "      <td>train</td>\n",
       "      <td>Tangents (better known to MST3K watchers as Ti...</td>\n",
       "      <td>unsup</td>\n",
       "      <td>9996_0.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99994</th>\n",
       "      <td>99994</td>\n",
       "      <td>train</td>\n",
       "      <td>Time Chasers joins various films that have bee...</td>\n",
       "      <td>unsup</td>\n",
       "      <td>9997_0.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99995</th>\n",
       "      <td>99995</td>\n",
       "      <td>train</td>\n",
       "      <td>Delightfully awful! Made by David Giancola, a ...</td>\n",
       "      <td>unsup</td>\n",
       "      <td>9998_0.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99996</th>\n",
       "      <td>99996</td>\n",
       "      <td>train</td>\n",
       "      <td>Watching Time Chasers, it obvious that it was ...</td>\n",
       "      <td>unsup</td>\n",
       "      <td>9999_0.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99997</th>\n",
       "      <td>99997</td>\n",
       "      <td>train</td>\n",
       "      <td>At the beginning we can see members of Troma t...</td>\n",
       "      <td>unsup</td>\n",
       "      <td>999_0.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99998</th>\n",
       "      <td>99998</td>\n",
       "      <td>train</td>\n",
       "      <td>The movie was incredible, ever since I saw it ...</td>\n",
       "      <td>unsup</td>\n",
       "      <td>99_0.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99999</th>\n",
       "      <td>99999</td>\n",
       "      <td>train</td>\n",
       "      <td>TCM came through by acquiring this wonderful, ...</td>\n",
       "      <td>unsup</td>\n",
       "      <td>9_0.txt</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100000 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0   type                                             review  \\\n",
       "0               0   test  Once again Mr. Costner has dragged out a movie...   \n",
       "1               1   test  This is an example of why the majority of acti...   \n",
       "2               2   test  First of all I hate those moronic rappers, who...   \n",
       "3               3   test  Not even the Beatles could write songs everyon...   \n",
       "4               4   test  Brass pictures (movies is not a fitting word f...   \n",
       "5               5   test  A funny thing happened to me while watching \"M...   \n",
       "6               6   test  This German horror film has to be one of the w...   \n",
       "7               7   test  Being a long-time fan of Japanese film, I expe...   \n",
       "8               8   test  \"Tokyo Eyes\" tells of a 17 year old Japanese g...   \n",
       "9               9   test  Wealthy horse ranchers in Buenos Aires have a ...   \n",
       "10             10   test  Cage plays a drunk and gets high critically pr...   \n",
       "11             11   test  First of all, I would like to say that I am a ...   \n",
       "12             12   test  So tell me - what serious boozer drinks Budwei...   \n",
       "13             13   test  A big disappointment for what was touted as an...   \n",
       "14             14   test  This film is absolutely appalling and awful. I...   \n",
       "15             15   test  Here's a decidedly average Italian post apocal...   \n",
       "16             16   test  At the bottom end of the apocalypse movie scal...   \n",
       "17             17   test  Earth has been destroyed in a nuclear holocaus...   \n",
       "18             18   test  Many people are standing in front of the house...   \n",
       "19             19   test  New York family is the last in their neighborh...   \n",
       "20             20   test  The best thing about \"The Prey\" is the tag lin...   \n",
       "21             21   test  This is truly, without exaggerating, one of th...   \n",
       "22             22   test  I'm a huge fan of both Emily Watson (Breaking ...   \n",
       "23             23   test  Sure, most of the slasher films of the 1980's ...   \n",
       "24             24   test  I think that would have been a more appropriat...   \n",
       "25             25   test  1980 was certainly a year for bad backwoods sl...   \n",
       "26             26   test  Everything everyone has said already pretty mu...   \n",
       "27             27   test  Uhhh ... so, did they even have writers for th...   \n",
       "28             28   test  Oh yeah, this one is definitely a strong conte...   \n",
       "29             29   test  Supercraptastic slasher fare, which feels over...   \n",
       "...           ...    ...                                                ...   \n",
       "99970       99970  train  This is without a doubt the worst movie I have...   \n",
       "99971       99971  train  Note: I've only seen the MST version of this f...   \n",
       "99972       99972  train  I've seen the MST3K version of this movie unde...   \n",
       "99973       99973  train  This disillusioning tale of time travel is mor...   \n",
       "99974       99974  train  All right, enough with the MST3K-ing, people. ...   \n",
       "99975       99975  train  I saw this in Pittsburgh PA in a small theater...   \n",
       "99976       99976  train  And then filmed during each actors day off at ...   \n",
       "99977       99977  train  Hey, it's not a time machine, it's a \"time tra...   \n",
       "99978       99978  train  This backyard movie has it all: time travel, a...   \n",
       "99979       99979  train  It was a true labor to sit through this \"movie...   \n",
       "99980       99980  train  The comments already posted for this film do n...   \n",
       "99981       99981  train  One of the funnier MST3K experiments. This was...   \n",
       "99982       99982  train  Time Chasers is probably one of the better mov...   \n",
       "99983       99983  train  ... to dull the incredible pain this stinkmast...   \n",
       "99984       99984  train  I feel badly for the actors and directors of t...   \n",
       "99985       99985  train  I'm not really sure they were trying to make a...   \n",
       "99986       99986  train  I got a screener of this film a couple of week...   \n",
       "99987       99987  train  ****SPOILERS****SPOILERS****SPOILERS****SPOILE...   \n",
       "99988       99988  train  This is a really dumb movie, with a very low b...   \n",
       "99989       99989  train  Nick Miller has developed a means of time trav...   \n",
       "99990       99990  train  That's what the director of this film had to b...   \n",
       "99991       99991  train  Think of this one as Back to the Future...exce...   \n",
       "99992       99992  train  What happened to the 'second' hero who went ba...   \n",
       "99993       99993  train  Tangents (better known to MST3K watchers as Ti...   \n",
       "99994       99994  train  Time Chasers joins various films that have bee...   \n",
       "99995       99995  train  Delightfully awful! Made by David Giancola, a ...   \n",
       "99996       99996  train  Watching Time Chasers, it obvious that it was ...   \n",
       "99997       99997  train  At the beginning we can see members of Troma t...   \n",
       "99998       99998  train  The movie was incredible, ever since I saw it ...   \n",
       "99999       99999  train  TCM came through by acquiring this wonderful, ...   \n",
       "\n",
       "       label         file  \n",
       "0        neg      0_2.txt  \n",
       "1        neg  10000_4.txt  \n",
       "2        neg  10001_1.txt  \n",
       "3        neg  10002_3.txt  \n",
       "4        neg  10003_3.txt  \n",
       "5        neg  10004_2.txt  \n",
       "6        neg  10005_2.txt  \n",
       "7        neg  10006_2.txt  \n",
       "8        neg  10007_4.txt  \n",
       "9        neg  10008_4.txt  \n",
       "10       neg  10009_3.txt  \n",
       "11       neg   1000_3.txt  \n",
       "12       neg  10010_2.txt  \n",
       "13       neg  10011_1.txt  \n",
       "14       neg  10012_1.txt  \n",
       "15       neg  10013_4.txt  \n",
       "16       neg  10014_2.txt  \n",
       "17       neg  10015_4.txt  \n",
       "18       neg  10016_3.txt  \n",
       "19       neg  10017_1.txt  \n",
       "20       neg  10018_1.txt  \n",
       "21       neg  10019_1.txt  \n",
       "22       neg   1001_4.txt  \n",
       "23       neg  10020_1.txt  \n",
       "24       neg  10021_3.txt  \n",
       "25       neg  10022_4.txt  \n",
       "26       neg  10023_4.txt  \n",
       "27       neg  10024_3.txt  \n",
       "28       neg  10025_2.txt  \n",
       "29       neg  10026_3.txt  \n",
       "...      ...          ...  \n",
       "99970  unsup   9975_0.txt  \n",
       "99971  unsup   9976_0.txt  \n",
       "99972  unsup   9977_0.txt  \n",
       "99973  unsup   9978_0.txt  \n",
       "99974  unsup   9979_0.txt  \n",
       "99975  unsup    997_0.txt  \n",
       "99976  unsup   9980_0.txt  \n",
       "99977  unsup   9981_0.txt  \n",
       "99978  unsup   9982_0.txt  \n",
       "99979  unsup   9983_0.txt  \n",
       "99980  unsup   9984_0.txt  \n",
       "99981  unsup   9985_0.txt  \n",
       "99982  unsup   9986_0.txt  \n",
       "99983  unsup   9987_0.txt  \n",
       "99984  unsup   9988_0.txt  \n",
       "99985  unsup   9989_0.txt  \n",
       "99986  unsup    998_0.txt  \n",
       "99987  unsup   9990_0.txt  \n",
       "99988  unsup   9991_0.txt  \n",
       "99989  unsup   9992_0.txt  \n",
       "99990  unsup   9993_0.txt  \n",
       "99991  unsup   9994_0.txt  \n",
       "99992  unsup   9995_0.txt  \n",
       "99993  unsup   9996_0.txt  \n",
       "99994  unsup   9997_0.txt  \n",
       "99995  unsup   9998_0.txt  \n",
       "99996  unsup   9999_0.txt  \n",
       "99997  unsup    999_0.txt  \n",
       "99998  unsup     99_0.txt  \n",
       "99999  unsup      9_0.txt  \n",
       "\n",
       "[100000 rows x 5 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "imdb = pd.read_csv('datasets/imdb_master.csv', encoding='latin-1')\n",
    "imdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_rows = imdb.loc[(imdb['type'] == 'train') & ((imdb['label'] == 'pos') | (imdb['label'] == 'neg'))]\n",
    "with open('datasets/reviews.txt', 'w') as f:\n",
    "    for r in train_rows['review']:\n",
    "        f.write(r+'\\n')\n",
    "        \n",
    "with open('datasets/labels.txt', 'w') as f:\n",
    "    for l in train_rows['label']:\n",
    "        f.write(l+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "f = open('datasets/reviews.txt')\n",
    "raw_reviews = f.readlines()\n",
    "f.close()\n",
    "\n",
    "f = open('datasets/labels.txt')\n",
    "raw_labels = f.readlines()\n",
    "f.close()\n",
    "\n",
    "tokens = list(map(lambda x:set(x.split(\" \")),raw_reviews))\n",
    "vocab = set()\n",
    "for sent in tokens:\n",
    "    for word in sent:\n",
    "        if(len(word)>0):\n",
    "            vocab.add(word)\n",
    "\n",
    "vocab = list(vocab)\n",
    "word2index = {}\n",
    "for i,word in enumerate(vocab):\n",
    "    word2index[word]=i\n",
    "input_dataset = list()\n",
    "for sent in tokens:\n",
    "    sent_indices = list()\n",
    "    for word in sent:\n",
    "        try:\n",
    "            sent_indices.append(word2index[word])\n",
    "        except:\n",
    "            pass\n",
    "    input_dataset.append(list(set(sent_indices)))\n",
    "\n",
    "target_dataset = list()\n",
    "for label in raw_labels:\n",
    "    if label == 'pos\\n':\n",
    "        target_dataset.append(1)\n",
    "    elif label == 'neg\\n':\n",
    "        target_dataset.append(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length dataset samples 25000\n",
      "Length positive samples 12500\n",
      "Length negative samples 12500\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils import shuffle\n",
    "input_dataset_shuffled, target_dataset_shuffled = shuffle(input_dataset, target_dataset)\n",
    "print('Length dataset samples {}'.format(len(input_dataset_shuffled)))\n",
    "print('Length positive samples {}'.format(len([t for t in target_dataset_shuffled if t == 1])))\n",
    "print('Length negative samples {}'.format(len([t for t in target_dataset_shuffled if t == 0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Emdedding Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter:0 Progress:95.99% Training Accuracy:0.8130416666666667%%\n",
      "Iter:1 Progress:95.99% Training Accuracy:0.8585833333333334%\n",
      "Iter:2 Progress:95.99% Training Accuracy:0.8870277777777777%\n",
      "Iter:3 Progress:95.99% Training Accuracy:0.9084166666666667%\n",
      "Iter:4 Progress:95.99% Training Accuracy:0.9245583333333334%\n",
      "Test Accuracy:0.864\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.random.seed(1)\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1/(1 + np.exp(-x))\n",
    "\n",
    "alpha, iterations = (0.01, 5)\n",
    "hidden_size = 100\n",
    "weights_0_1 = 0.2*np.random.random((len(vocab),hidden_size)) - 0.1\n",
    "weights_1_2 = 0.2*np.random.random((hidden_size,1)) - 0.1\n",
    "\n",
    "correct,total = (0,0)\n",
    "for iter in range(iterations):\n",
    "    # train on first 24,000\n",
    "    for i in range(len(input_dataset_shuffled)-1000):\n",
    "        x,y = (input_dataset_shuffled[i],target_dataset_shuffled[i])\n",
    "        layer_1 = sigmoid(np.sum(weights_0_1[x],axis=0)) #embed + sigmoid\n",
    "        layer_2 = sigmoid(np.dot(layer_1,weights_1_2)) # linear + softmax\n",
    "        layer_2_delta = layer_2 - y # compare pred with truth\n",
    "        layer_1_delta = layer_2_delta.dot(weights_1_2.T) #backprop\n",
    "        weights_0_1[x] -= layer_1_delta * alpha\n",
    "        weights_1_2 -= np.outer(layer_1,layer_2_delta) * alpha\n",
    "        \n",
    "        if(np.abs(layer_2_delta) < 0.5):\n",
    "            correct += 1\n",
    "        total += 1\n",
    "        if(i % 10 == 9):\n",
    "            progress = str(i/float(len(input_dataset_shuffled)))\n",
    "            sys.stdout.write('\\rIter:'+str(iter)\\\n",
    "            +' Progress:'+progress[2:4]\\\n",
    "            +'.'+progress[4:6]\\\n",
    "            +'% Training Accuracy:'\\\n",
    "            + str(correct/float(total)) + '%')\n",
    "    print()\n",
    "\n",
    "correct,total = (0,0)\n",
    "for i in range(len(input_dataset_shuffled)-1000,len(input_dataset_shuffled)):\n",
    "    x = input_dataset_shuffled[i]\n",
    "    y = target_dataset_shuffled[i]\n",
    "    layer_1 = sigmoid(np.sum(weights_0_1[x],axis=0))\n",
    "    layer_2 = sigmoid(np.dot(layer_1,weights_1_2))\n",
    "    if(np.abs(layer_2 - y) < 0.5):\n",
    "        correct += 1\n",
    "    total += 1\n",
    "print(\"Test Accuracy:\" + str(correct / float(total)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 12.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import Counter\n",
    "import math\n",
    "\n",
    "norms = np.sum(weights_0_1 * weights_0_1,axis=1)\n",
    "norms.resize(norms.shape[0],1)\n",
    "normed_weights = weights_0_1 * norms\n",
    "\n",
    "def make_sent_vect(words):\n",
    "    indices = list(map(lambda x:word2index[x], filter(lambda x:x in word2index,words)))\n",
    "    return np.mean(normed_weights[indices],axis=0)\n",
    "\n",
    "reviews2vectors = list()\n",
    "for review in tokens: # tokenized reviews\n",
    "    reviews2vectors.append(make_sent_vect(review))\n",
    "reviews2vectors = np.array(reviews2vectors)\n",
    "\n",
    "def most_similar_reviews(review):\n",
    "    v = make_sent_vect(review)\n",
    "    scores = Counter()\n",
    "    for i,val in enumerate(reviews2vectors.dot(v)):\n",
    "        scores[i] = val\n",
    "    most_similar = list()\n",
    "    for idx,score in scores.most_common(3):\n",
    "        most_similar.append(raw_reviews[idx][0:100])\n",
    "    return most_similar\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"The characters are unlikeable and the script is awful. It's a waste of the talents of Deneuve and Au\",\n",
       " 'This movie is so bad, it can only be compared to the all-time worst \"comedy\": Police Academy 7. No l',\n",
       " 'Most definitely the worst Columbo ever dreamt up. No murder and the abandonment of the tried and tes']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_similar_reviews(['boring','awful'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Excellent episode movie ala Pulp Fiction. 7 days - 7 suicides. It doesnt get more depressing than th',\n",
       " 'Smallville episode Justice is the best episode of Smallville ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! !',\n",
       " 'Smallville episode Justice is the best episode of Smallville ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! !']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_similar_reviews(['great','amazing'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 12.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]]\n",
      "[1. 2. 3.]\n",
      "[0.1 0.2 0.3]\n",
      "[-1.  -0.5  0. ]\n",
      "[0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "a = np.array([1,2,3])\n",
    "b = np.array([0.1,0.2,0.3])\n",
    "c = np.array([-1,-0.5,0])\n",
    "d = np.array([0,0,0])\n",
    "\n",
    "identity = np.eye(3)\n",
    "print(identity)\n",
    "\n",
    "print(a.dot(identity))\n",
    "print(b.dot(identity))\n",
    "print(c.dot(identity))\n",
    "print(d.dot(identity))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13 15 17]\n",
      "[13. 15. 17.]\n"
     ]
    }
   ],
   "source": [
    "this = np.array([2,4,6])\n",
    "movie = np.array([10,10,10])\n",
    "rocks = np.array([1,1,1])\n",
    "print(this + movie + rocks)\n",
    "print((this.dot(identity) + movie).dot(identity) + rocks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 12.11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.11111111 0.11111111 0.11111111 0.11111111 0.11111111 0.11111111\n",
      "  0.11111111 0.11111111 0.11111111]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def softmax(_x):\n",
    "    x = np.atleast_2d(_x)\n",
    "    temp = np.exp(x)\n",
    "    return temp / np.sum(temp, axis=1, keepdims=True)\n",
    "\n",
    "word_vects = {}\n",
    "word_vects['yankees'] = np.array([[0., 0., 0.]])\n",
    "word_vects['bears'] = np.array([[0., 0., 0.]])\n",
    "word_vects['braves'] = np.array([[0., 0., 0.]])\n",
    "word_vects['red'] = np.array([[0., 0., 0.]])\n",
    "word_vects['socks'] = np.array([[0., 0., 0.]])\n",
    "word_vects['lose'] = np.array([[0., 0., 0.]])\n",
    "word_vects['defeat'] = np.array([[0., 0., 0.]])\n",
    "word_vects['beat'] = np.array([[0., 0., 0.]])\n",
    "word_vects['tie'] = np.array([[0., 0., 0.]])\n",
    "\n",
    "sent2output = np.random.rand(3, len(word_vects))\n",
    "identity = np.eye(3)\n",
    "\n",
    "layer_0 = word_vects['red']\n",
    "layer_1 = np.dot(layer_0, identity) + word_vects['socks']\n",
    "layer_2 = np.dot(layer_1, identity) + word_vects['defeat']\n",
    "\n",
    "pred = softmax(np.dot(layer_2, sent2output))\n",
    "print(pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 12.12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.array([1,0,0,0,0,0,0,0,0]) # target one-hot vector for \"yankees\"\n",
    "pred_delta = pred - y\n",
    "layer_2_delta = pred_delta.dot(sent2output.T)\n",
    "defeat_delta = layer_2_delta * 1 # can ignore the \"1\" like prev. chapter\n",
    "layer_1_delta = layer_2_delta.dot(identity.T)\n",
    "socks_delta = layer_1_delta * 1 # again... can ignore the \"1\"\n",
    "layer_0_delta = layer_1_delta.dot(identity.T)\n",
    "alpha = 0.01\n",
    "word_vects['red'] -= layer_0_delta * alpha\n",
    "word_vects['socks'] -= socks_delta * alpha\n",
    "word_vects['defeat'] -= defeat_delta * alpha\n",
    "identity -= np.outer(layer_0,layer_1_delta) * alpha\n",
    "identity -= np.outer(layer_1,layer_2_delta) * alpha\n",
    "sent2output -= np.outer(layer_2,pred_delta) * alpha"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 12.13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! wget http://www.thespermwhale.com/jaseweston/babi/tasks_1-20_v1-1.tar.gz\n",
    "! tar -xvf tasks_1-20_v1-1.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['mary', 'moved', 'to', 'the', 'bathroom.'], ['john', 'went', 'to', 'the', 'hallway.'], ['where', 'is', 'mary?', '\\tbathroom\\t1']]\n"
     ]
    }
   ],
   "source": [
    "import sys,random,math\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "\n",
    "f = open('datasets/tasksv11/en/qa1_single-supporting-fact_train.txt','r')\n",
    "raw = f.readlines()\n",
    "f.close()\n",
    "tokens = list()\n",
    "\n",
    "for line in raw[0:1000]:\n",
    "    tokens.append(line.lower().replace(\"\\n\",\"\").split(\" \")[1:])\n",
    "print(tokens[0:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = set()\n",
    "for sent in tokens:\n",
    "    for word in sent:\n",
    "        vocab.add(word)\n",
    "vocab = list(vocab)\n",
    "\n",
    "word2index = {}\n",
    "for i,word in enumerate(vocab):\n",
    "    word2index[word]=i\n",
    "\n",
    "def words2indices(sentence):\n",
    "    idx = list()\n",
    "    for word in sentence:\n",
    "        idx.append(word2index[word])\n",
    "    return idx\n",
    "\n",
    "def softmax(x):\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return e_x / e_x.sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "\n",
    "embed_size = 10\n",
    "# word embeddings\n",
    "embed = (np.random.rand(len(vocab),embed_size) - 0.5) * 0.1\n",
    "# embedding -> embedding (initially the identity matrix)\n",
    "recurrent = np.eye(embed_size)\n",
    "# sentence embedding for empty sentence\n",
    "start = np.zeros(embed_size)\n",
    "# embedding -> output weights\n",
    "decoder = (np.random.rand(embed_size, len(vocab)) - 0.5) * 0.1\n",
    "# one hot lookups (for loss function)\n",
    "one_hot = np.eye(len(vocab))\n",
    "\n",
    "def predict(sent):\n",
    "    layers = list()\n",
    "    layer = {}\n",
    "    layer['hidden'] = start\n",
    "    layers.append(layer)\n",
    "    loss = 0\n",
    "    # forward propagate\n",
    "    preds = list()\n",
    "    for target_i in range(len(sent)):\n",
    "        layer = {}\n",
    "        # try to predict the next term\n",
    "        layer['pred'] = softmax(layers[-1]['hidden'].dot(decoder))\n",
    "        loss += -np.log(layer['pred'][sent[target_i]])\n",
    "        # generate the next hidden state\n",
    "        layer['hidden'] = layers[-1]['hidden'].dot(recurrent) + embed[sent[target_i]]\n",
    "        layers.append(layer)\n",
    "    return layers, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity:82.01968947036016\n",
      "Perplexity:82.01968947036016\n",
      "Perplexity:82.01968947036016\n",
      "Perplexity:82.01968947036016\n",
      "Perplexity:81.88890996222612\n",
      "Perplexity:81.88890996222612\n",
      "Perplexity:81.88890996222612\n",
      "Perplexity:81.88890996222612\n",
      "Perplexity:81.66944062593234\n",
      "Perplexity:81.66944062593234\n",
      "Perplexity:81.66944062593234\n",
      "Perplexity:81.66944062593234\n",
      "Perplexity:81.2533657132904\n",
      "Perplexity:81.2533657132904\n",
      "Perplexity:81.2533657132904\n",
      "Perplexity:81.2533657132904\n",
      "Perplexity:80.41260197960652\n",
      "Perplexity:80.41260197960652\n",
      "Perplexity:80.41260197960652\n",
      "Perplexity:80.41260197960652\n",
      "Perplexity:78.57213147957657\n",
      "Perplexity:78.57213147957657\n",
      "Perplexity:78.57213147957657\n",
      "Perplexity:78.57213147957657\n",
      "Perplexity:73.79275536232552\n",
      "Perplexity:73.79275536232552\n",
      "Perplexity:73.79275536232552\n",
      "Perplexity:73.79275536232552\n",
      "Perplexity:54.61649580473904\n",
      "Perplexity:54.61649580473904\n",
      "Perplexity:54.61649580473904\n",
      "Perplexity:54.61649580473904\n",
      "Perplexity:28.740329201730873\n",
      "Perplexity:28.740329201730873\n",
      "Perplexity:28.740329201730873\n",
      "Perplexity:28.740329201730873\n",
      "Perplexity:20.881041157461038\n",
      "Perplexity:20.881041157461038\n",
      "Perplexity:20.881041157461038\n",
      "Perplexity:20.881041157461038\n",
      "Perplexity:19.43145544016516\n",
      "Perplexity:19.43145544016516\n",
      "Perplexity:19.43145544016516\n",
      "Perplexity:19.43145544016516\n",
      "Perplexity:18.493500846681826\n",
      "Perplexity:18.493500846681826\n",
      "Perplexity:18.493500846681826\n",
      "Perplexity:18.493500846681826\n",
      "Perplexity:17.464132138722544\n",
      "Perplexity:17.464132138722544\n",
      "Perplexity:17.464132138722544\n",
      "Perplexity:17.464132138722544\n",
      "Perplexity:16.035831899626395\n",
      "Perplexity:16.035831899626395\n",
      "Perplexity:16.035831899626395\n",
      "Perplexity:16.035831899626395\n",
      "Perplexity:13.730755832490603\n",
      "Perplexity:13.730755832490603\n",
      "Perplexity:13.730755832490603\n",
      "Perplexity:13.730755832490603\n",
      "Perplexity:10.48765574438247\n",
      "Perplexity:10.48765574438247\n",
      "Perplexity:10.48765574438247\n",
      "Perplexity:10.48765574438247\n",
      "Perplexity:7.884789234784629\n",
      "Perplexity:7.884789234784629\n",
      "Perplexity:7.884789234784629\n",
      "Perplexity:7.884789234784629\n",
      "Perplexity:6.621046565960203\n",
      "Perplexity:6.621046565960203\n",
      "Perplexity:6.621046565960203\n",
      "Perplexity:6.621046565960203\n",
      "Perplexity:5.833279998475154\n",
      "Perplexity:5.833279998475154\n",
      "Perplexity:5.833279998475154\n",
      "Perplexity:5.833279998475154\n",
      "Perplexity:5.345181778343352\n",
      "Perplexity:5.345181778343352\n",
      "Perplexity:5.345181778343352\n",
      "Perplexity:5.345181778343352\n",
      "Perplexity:4.995825539081375\n",
      "Perplexity:4.995825539081375\n",
      "Perplexity:4.995825539081375\n",
      "Perplexity:4.995825539081375\n",
      "Perplexity:4.747211478515534\n",
      "Perplexity:4.747211478515534\n",
      "Perplexity:4.747211478515534\n",
      "Perplexity:4.747211478515534\n",
      "Perplexity:4.575414711962951\n",
      "Perplexity:4.575414711962951\n",
      "Perplexity:4.575414711962951\n",
      "Perplexity:4.575414711962951\n",
      "Perplexity:4.464456098523182\n",
      "Perplexity:4.464456098523182\n",
      "Perplexity:4.464456098523182\n",
      "Perplexity:4.464456098523182\n",
      "Perplexity:4.389793946308148\n",
      "Perplexity:4.389793946308148\n",
      "Perplexity:4.389793946308148\n",
      "Perplexity:4.389793946308148\n",
      "Perplexity:4.32356221880104\n",
      "Perplexity:4.32356221880104\n",
      "Perplexity:4.32356221880104\n",
      "Perplexity:4.32356221880104\n",
      "Perplexity:4.248019256291583\n",
      "Perplexity:4.248019256291583\n",
      "Perplexity:4.248019256291583\n",
      "Perplexity:4.248019256291583\n",
      "Perplexity:4.155869932886341\n",
      "Perplexity:4.155869932886341\n",
      "Perplexity:4.155869932886341\n",
      "Perplexity:4.155869932886341\n",
      "Perplexity:4.046775743838573\n",
      "Perplexity:4.046775743838573\n",
      "Perplexity:4.046775743838573\n",
      "Perplexity:4.046775743838573\n",
      "Perplexity:3.9257655762721337\n",
      "Perplexity:3.9257655762721337\n",
      "Perplexity:3.9257655762721337\n",
      "Perplexity:3.9257655762721337\n"
     ]
    }
   ],
   "source": [
    "# forward\n",
    "for iter in range(30000):\n",
    "    alpha = 0.001\n",
    "    sent = words2indices(tokens[iter%len(tokens)][1:])\n",
    "    layers,loss = predict(sent)\n",
    "    # back propagate\n",
    "    for layer_idx in reversed(range(len(layers))):\n",
    "        layer = layers[layer_idx]\n",
    "        target = sent[layer_idx-1]\n",
    "        if(layer_idx > 0): # if not the first layer\n",
    "            layer['output_delta'] = layer['pred'] - one_hot[target]\n",
    "            new_hidden_delta = layer['output_delta']\\\n",
    "            .dot(decoder.transpose())\n",
    "            # if the last layer - don't pull from a\n",
    "            # later one becasue it doesn't exist\n",
    "            if(layer_idx == len(layers)-1):\n",
    "                layer['hidden_delta'] = new_hidden_delta\n",
    "            else:\n",
    "                layer['hidden_delta'] = new_hidden_delta + \\\n",
    "                layers[layer_idx+1]['hidden_delta']\\\n",
    "                .dot(recurrent.transpose())\n",
    "        else: # if the first layer\n",
    "            layer['hidden_delta'] = layers[layer_idx+1]['hidden_delta']\\\n",
    "            .dot(recurrent.transpose())\n",
    "            \n",
    "    # update weights\n",
    "    start -= layers[0]['hidden_delta'] * alpha / float(len(sent))\n",
    "    for layer_idx,layer in enumerate(layers[1:]):\n",
    "        decoder -= np.outer(layers[layer_idx]['hidden'],\\\n",
    "        layer['output_delta']) * alpha / float(len(sent))\n",
    "        embed_idx = sent[layer_idx]\n",
    "        embed[embed_idx] -= layers[layer_idx]['hidden_delta'] * \\\n",
    "        alpha / float(len(sent))\n",
    "        recurrent -= np.outer(layers[layer_idx]['hidden'],\\\n",
    "        layer['hidden_delta']) * alpha / float(len(sent))\n",
    "        if(iter % 1000 == 0):\n",
    "            print(\"Perplexity:\" + str(np.exp(loss/len(sent))))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section  12.18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sandra', 'moved', 'to', 'the', 'garden.']\n",
      "Prev Input:sandra      True:moved          Pred:is\n",
      "Prev Input:moved       True:to             Pred:to\n",
      "Prev Input:to          True:the            Pred:the\n",
      "Prev Input:the         True:garden.        Pred:bedroom.\n"
     ]
    }
   ],
   "source": [
    "sent_index = 4\n",
    "l,_ = predict(words2indices(tokens[sent_index]))\n",
    "print(tokens[sent_index])\n",
    "for i,each_layer in enumerate(l[1:-1]):\n",
    "    input = tokens[sent_index][i]\n",
    "    true = tokens[sent_index][i+1]\n",
    "    pred = vocab[each_layer['pred'].argmax()]\n",
    "    print(\"Prev Input:\" + input + (' ' * (12 - len(input))) +\\\n",
    "    \"True:\" + true + (\" \" * (15 - len(true))) + \"Pred:\" + pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
