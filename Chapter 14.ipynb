{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning to Write Like Shakespeare\n",
    "## Long-Short Term Memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 14.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys,random,math\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import sys\n",
    "np.random.seed(0)\n",
    "\n",
    "f = open('datasets/shakespear.txt','r')\n",
    "# from http://karpathy.github.io/2015/05/21/rnn-effectiveness/\n",
    "raw = f.read()\n",
    "f.close()\n",
    "vocab = list(set(raw))\n",
    "word2index = {}\n",
    "for i,word in enumerate(vocab):\n",
    "    word2index[word]=i\n",
    "indices = np.array(list(map(lambda x:word2index[x], raw)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Framework Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Tensor (object):\n",
    "    def __init__(self,data, autograd=False, creators=None, creation_op=None, id=None):\n",
    "        self.data = np.array(data)\n",
    "        self.creators = creators\n",
    "        self.creation_op = creation_op\n",
    "        self.grad = None\n",
    "        self.autograd = autograd\n",
    "        self.children = {}\n",
    "        self.is_recurrent = False\n",
    "        if(id is None):\n",
    "            id = np.random.randint(0,100000)\n",
    "        self.id = id\n",
    "        if(creators is not None):\n",
    "            for c in creators:\n",
    "                if(self.id not in c.children):\n",
    "                    c.children[self.id] = 1\n",
    "                else:\n",
    "                    c.children[self.id] += 1\n",
    "   \n",
    "    def all_children_grads_accounted_for(self):\n",
    "        for id,cnt in self.children.items():\n",
    "            if(cnt != 0):\n",
    "                return False\n",
    "        return True\n",
    "   \n",
    "    def backward(self,grad=None, grad_origin=None):\n",
    "        if(self.autograd):\n",
    "            \n",
    "            if(grad is None):\n",
    "                grad = Tensor(np.ones_like(self.data))\n",
    "            \n",
    "            if(grad_origin is not None):\n",
    "                if (self.children[grad_origin.id] == 0):\n",
    "                    return\n",
    "                else:\n",
    "                    self.children[grad_origin.id] -= 1\n",
    "            if(self.grad is None):\n",
    "                self.grad = grad\n",
    "            else:\n",
    "                self.grad += grad\n",
    "                \n",
    "            if(self.creators is not None and (self.all_children_grads_accounted_for() or grad_origin is None)):\n",
    "                if(self.creation_op == \"add\"):\n",
    "                    self.creators[0].backward(self.grad, self)\n",
    "                    self.creators[1].backward(self.grad, self)\n",
    "                if(self.creation_op == \"neg\"):\n",
    "                    self.creators[0].backward(self.grad.__neg__())\n",
    "                if(self.creation_op == \"sub\"):\n",
    "                    new = Tensor(self.grad.data)\n",
    "                    self.creators[0].backward(new, self)\n",
    "                    new = Tensor(self.grad.__neg__().data)\n",
    "                    self.creators[1].backward(new, self)\n",
    "                if(self.creation_op == \"mul\"):\n",
    "                    new = self.grad * self.creators[1]\n",
    "                    self.creators[0].backward(new, self)\n",
    "                    new = self.grad * self.creators[0]\n",
    "                    self.creators[1].backward(new, self)\n",
    "                if(self.creation_op == \"mm\"):\n",
    "                    act = self.creators[0] # usually an activation\n",
    "                    weights = self.creators[1] # usually a weight matrix\n",
    "                    new = self.grad.mm(weights.transpose())\n",
    "                    act.backward(new)\n",
    "                    new = self.grad.transpose().mm(act).transpose()\n",
    "                    weights.backward(new)\n",
    "                if(self.creation_op == \"transpose\"):\n",
    "                    self.creators[0].backward(self.grad.transpose())\n",
    "                if(\"sum\" in self.creation_op):\n",
    "                    dim = int(self.creation_op.split(\"_\")[1])\n",
    "                    ds = self.creators[0].data.shape[dim]\n",
    "                    self.creators[0].backward(self.grad.expand(dim,ds))\n",
    "                if(\"expand\" in self.creation_op):\n",
    "                    dim = int(self.creation_op.split(\"_\")[1])\n",
    "                    self.creators[0].backward(self.grad.sum(dim))\n",
    "                if(self.creation_op == \"sigmoid\"):\n",
    "                    ones = Tensor(np.ones_like(self.grad.data))\n",
    "                    self.creators[0].backward(self.grad * (self * (ones - self)))\n",
    "                if(self.creation_op == \"tanh\"):\n",
    "                    ones = Tensor(np.ones_like(self.grad.data))\n",
    "                    self.creators[0].backward(self.grad * (ones - (self * self)))\n",
    "                if(self.creation_op == \"index_select\"):\n",
    "                    new_grad = np.zeros_like(self.creators[0].data)\n",
    "                    indices_ = self.index_select_indices.data.flatten()\n",
    "                    grad_ = grad.data.reshape(len(indices_), -1)\n",
    "                    for i in range(len(indices_)):\n",
    "                        new_grad[indices_[i]] += grad_[i]\n",
    "                    self.creators[0].backward(Tensor(new_grad))\n",
    "                if(self.creation_op == \"cross_entropy\"):\n",
    "                    dx = self.softmax_output - self.target_dist\n",
    "                    self.creators[0].backward(Tensor(dx))\n",
    "    \n",
    "    def __add__(self, other):\n",
    "        if(self.autograd and other.autograd):\n",
    "            return Tensor(self.data + other.data, autograd=True, creators=[self,other], creation_op=\"add\")\n",
    "        return Tensor(self.data + other.data)\n",
    "   \n",
    "    def __neg__(self):\n",
    "        if(self.autograd):\n",
    "            return Tensor(self.data * -1, autograd=True, creators=[self], creation_op=\"neg\")\n",
    "        return Tensor(self.data * -1)\n",
    "    \n",
    "    def __sub__(self, other):\n",
    "        if(self.autograd and other.autograd): \n",
    "            return Tensor(self.data - other.data, autograd=True, creators=[self,other], creation_op=\"sub\")\n",
    "        return Tensor(self.data - other.data)\n",
    "\n",
    "    def __mul__(self, other):\n",
    "        if(self.autograd and other.autograd):\n",
    "            return Tensor(self.data * other.data, autograd=True, creators=[self,other], creation_op=\"mul\")\n",
    "        return Tensor(self.data * other.data)\n",
    "    \n",
    "    def __truediv__(self, other):\n",
    "        if(self.autograd and other.autograd):\n",
    "            return Tensor(self.data / other.data, autograd=True, creators=[self,other], creation_op=\"div\")\n",
    "        return Tensor(self.data / other.data)\n",
    "\n",
    "    def sum(self, dim):\n",
    "        if(self.autograd):\n",
    "            return Tensor(self.data.sum(dim), autograd=True, creators=[self], creation_op=\"sum_\"+str(dim))\n",
    "        return Tensor(self.data.sum(dim))\n",
    "\n",
    "    def expand(self, dim,copies):\n",
    "        trans_cmd = list(range(0,len(self.data.shape)))\n",
    "        trans_cmd.insert(dim,len(self.data.shape))\n",
    "        new_shape = list(self.data.shape) + [copies]\n",
    "        new_data = self.data.repeat(copies).reshape(new_shape)\n",
    "        new_data = new_data.transpose(trans_cmd)\n",
    "        if(self.autograd):\n",
    "            return Tensor(new_data, autograd=True, creators=[self], creation_op=\"expand_\"+str(dim))\n",
    "        return Tensor(new_data)\n",
    "    \n",
    "    def transpose(self):\n",
    "        if(self.autograd):\n",
    "            return Tensor(self.data.transpose(), autograd=True, creators=[self], creation_op=\"transpose\")\n",
    "        return Tensor(self.data.transpose())\n",
    "\n",
    "    def mm(self, x):\n",
    "        if(self.autograd):\n",
    "            return Tensor(self.data.dot(x.data), autograd=True, creators=[self,x], creation_op=\"mm\")\n",
    "        return Tensor(self.data.dot(x.data))\n",
    "    \n",
    "    def sigmoid(self):\n",
    "        if(self.autograd):\n",
    "            return Tensor(1 / (1 + np.exp(-self.data)),\n",
    "            autograd=True,\n",
    "            creators=[self],\n",
    "            creation_op=\"sigmoid\")\n",
    "        return Tensor(1 / (1 + np.exp(-self.data)))\n",
    "\n",
    "    def tanh(self):\n",
    "        if(self.autograd):\n",
    "            return Tensor(np.tanh(self.data),\n",
    "            autograd=True,\n",
    "            creators=[self],\n",
    "            creation_op=\"tanh\")\n",
    "        return Tensor(np.tanh(self.data))\n",
    "    \n",
    "    \n",
    "    def softmax(self):\n",
    "        e_x = np.exp(self.data - np.max(self.data))\n",
    "        return e_x / e_x.sum(axis=0)\n",
    "    \n",
    "    def index_select(self, indices):\n",
    "        if(self.autograd):\n",
    "            new = Tensor(self.data[indices.data],\n",
    "            autograd=True,\n",
    "            creators=[self],\n",
    "            creation_op=\"index_select\")\n",
    "            new.index_select_indices = indices\n",
    "            return new\n",
    "        return Tensor(self.data[indices.data])\n",
    "    \n",
    "    def cross_entropy(self, target_indices):\n",
    "        temp = np.exp(self.data)\n",
    "        softmax_output = temp / np.sum(temp,\n",
    "        axis=len(self.data.shape)-1,\n",
    "        keepdims=True)\n",
    "        t = target_indices.data.flatten()\n",
    "        p = softmax_output.reshape(len(t),-1)\n",
    "    \n",
    "        target_dist = np.eye(p.shape[1])[t]\n",
    "        loss = -(np.log(p) * (target_dist)).sum(1).mean()\n",
    "        if(self.autograd):\n",
    "            out = Tensor(loss,\n",
    "            autograd=True,\n",
    "            creators=[self],\n",
    "            creation_op=\"cross_entropy\")\n",
    "            out.softmax_output = softmax_output\n",
    "            out.target_dist = target_dist\n",
    "            return out\n",
    "        return Tensor(loss)\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return str(self.data.__repr__())\n",
    "    def __str__(self):\n",
    "        return str(self.data.__str__())\n",
    "\n",
    "class Layer(object):\n",
    "    def __init__(self):\n",
    "        self.parameters = list()\n",
    "    def get_parameters(self):\n",
    "        return self.parameters\n",
    "\n",
    "class Linear(Layer):\n",
    "    def __init__(self, n_inputs, n_outputs, bias=True):\n",
    "        super().__init__()\n",
    "        W = np.random.randn(n_inputs, n_outputs)*np.sqrt(2.0/(n_inputs))\n",
    "        self.weight = Tensor(W, autograd=True)\n",
    "        self.parameters.append(self.weight)\n",
    "        if bias:\n",
    "            self.bias = Tensor(np.zeros(n_outputs), autograd=True)\n",
    "            self.parameters.append(self.bias)\n",
    "        else:\n",
    "            self.bias = None\n",
    "    \n",
    "    def forward(self, input):\n",
    "        temp = input.mm(self.weight)\n",
    "        if self.bias:\n",
    "            temp += self.bias.expand(0,len(input.data))\n",
    "        return temp\n",
    "    \n",
    "class Embedding(Layer):\n",
    "    def __init__(self, vocab_size, dim):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.dim = dim\n",
    "        # this initialiation style is just a convention from word2vec\n",
    "        weight = (np.random.rand(vocab_size, dim) - 0.5) / dim\n",
    "        self.weight = Tensor(weight, autograd=True)\n",
    "        self.parameters.append(self.weight)\n",
    "    def forward(self, input):\n",
    "         return self.weight.index_select(input)\n",
    "    \n",
    "class CrossEntropyLoss(object):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    def forward(self, input, target):\n",
    "        return input.cross_entropy(target)\n",
    "\n",
    "class RNNCell(Layer):\n",
    "    def __init__(self, n_inputs,n_hidden,n_output,activation='sigmoid'):\n",
    "        super().__init__()\n",
    "        self.n_inputs = n_inputs\n",
    "        self.n_hidden = n_hidden\n",
    "        self.n_output = n_output\n",
    "        if(activation == 'sigmoid'):\n",
    "            self.activation = Sigmoid()\n",
    "        elif(activation == 'tanh'):\n",
    "            self.activation == Tanh()\n",
    "        else:\n",
    "            raise Exception(\"Non-linearity not found\")\n",
    "        self.w_ih = Linear(n_inputs, n_hidden)\n",
    "        self.w_hh = Linear(n_hidden, n_hidden)\n",
    "        self.w_ho = Linear(n_hidden, n_output)\n",
    "        self.parameters += self.w_ih.get_parameters()\n",
    "        self.parameters += self.w_hh.get_parameters()\n",
    "        self.parameters += self.w_ho.get_parameters()\n",
    "        \n",
    "    def forward(self, input, hidden):\n",
    "        from_prev_hidden = self.w_hh.forward(hidden)\n",
    "        combined = self.w_ih.forward(input) + from_prev_hidden\n",
    "        new_hidden = self.activation.forward(combined)\n",
    "        output = self.w_ho.forward(new_hidden)\n",
    "        return output, new_hidden\n",
    "    def init_hidden(self, batch_size=1):\n",
    "        return Tensor(np.zeros((batch_size,self.n_hidden)),autograd=True)\n",
    "\n",
    "class Sigmoid(Layer):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    def forward(self, input):\n",
    "        return input.sigmoid()\n",
    "\n",
    "class SGD(object):\n",
    "    def __init__(self, parameters, alpha=0.1):\n",
    "        self.parameters = parameters\n",
    "        self.alpha = alpha\n",
    "    def zero(self):\n",
    "        for p in self.parameters:\n",
    "            p.grad.data *= 0\n",
    "    def step(self, zero=True):\n",
    "        for p in self.parameters:\n",
    "            p.data -= p.grad.data * self.alpha\n",
    "            if(zero):\n",
    "                p.grad.data *= 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed = Embedding(vocab_size=len(vocab),dim=512)\n",
    "model = RNNCell(n_inputs=512, n_hidden=512, n_output=len(vocab))\n",
    "criterion = CrossEntropyLoss()\n",
    "optim = SGD(parameters=model.get_parameters() + embed.get_parameters(), alpha=0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setion 14.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "bptt = 16\n",
    "n_batches = int((indices.shape[0] / (batch_size)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "trimmed_indices = indices[:n_batches*batch_size]\n",
    "batched_indices = trimmed_indices.reshape(batch_size, n_batches)\n",
    "batched_indices = batched_indices.transpose()\n",
    "input_batched_indices = batched_indices[0:-1]\n",
    "target_batched_indices = batched_indices[1:]\n",
    "n_bptt = int(((n_batches-1) / bptt))\n",
    "input_batches = input_batched_indices[:n_bptt*bptt]\n",
    "input_batches = input_batches.reshape(n_bptt,bptt,batch_size)\n",
    "target_batches = target_batched_indices[:n_bptt*bptt]\n",
    "target_batches = target_batches.reshape(n_bptt, bptt, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That,\n",
      "[52 46  3 58 45]\n"
     ]
    }
   ],
   "source": [
    "print(raw[0:5])\n",
    "print(indices[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[52  9 61 57 51 46 52  6  6  3  3 59 51  6 45 24  6 25 46 61 59  3  6  6\n",
      "   6 40  6 40 40  6  6  4]\n",
      " [46 24 24 57  2 59  3 35 25  4 37 59 37 58  6 24 15 51 51 24 37 14 15 34\n",
      "  46  4 28  0  4  0  5  6]\n",
      " [ 3 24 49 59 59  3 37 12 51  6 37 61 34 46 17 23 59 35 37 24 34 59 56 59\n",
      "  59 30 51 45  6 51 51 15]\n",
      " [58 48 52 56  6 56 15  6 35  4  6  6  9 59 59 16 59 45 25 29 12  6 51 59\n",
      "  56 37 35 24 19 56  0 51]\n",
      " [45 16 40 49 17  6 51 56  6 51 17 41 24  6  6 38  4  6  6 51  9 25 17  2\n",
      "  45  3 37 16 35 59 59 25]]\n"
     ]
    }
   ],
   "source": [
    "print(batched_indices[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[52  9 61 57 51 46 52  6  6  3  3 59 51  6 45 24  6 25 46 61 59  3  6  6\n",
      "   6 40  6 40 40  6  6  4]\n",
      " [46 24 24 57  2 59  3 35 25  4 37 59 37 58  6 24 15 51 51 24 37 14 15 34\n",
      "  46  4 28  0  4  0  5  6]\n",
      " [ 3 24 49 59 59  3 37 12 51  6 37 61 34 46 17 23 59 35 37 24 34 59 56 59\n",
      "  59 30 51 45  6 51 51 15]\n",
      " [58 48 52 56  6 56 15  6 35  4  6  6  9 59 59 16 59 45 25 29 12  6 51 59\n",
      "  56 37 35 24 19 56  0 51]\n",
      " [45 16 40 49 17  6 51 56  6 51 17 41 24  6  6 38  4  6  6 51  9 25 17  2\n",
      "  45  3 37 16 35 59 59 25]]\n",
      "[[46 24 24 57  2 59  3 35 25  4 37 59 37 58  6 24 15 51 51 24 37 14 15 34\n",
      "  46  4 28  0  4  0  5  6]\n",
      " [ 3 24 49 59 59  3 37 12 51  6 37 61 34 46 17 23 59 35 37 24 34 59 56 59\n",
      "  59 30 51 45  6 51 51 15]\n",
      " [58 48 52 56  6 56 15  6 35  4  6  6  9 59 59 16 59 45 25 29 12  6 51 59\n",
      "  56 37 35 24 19 56  0 51]\n",
      " [45 16 40 49 17  6 51 56  6 51 17 41 24  6  6 38  4  6  6 51  9 25 17  2\n",
      "  45  3 37 16 35 59 59 25]\n",
      " [ 6 42 12 34 40 59 58 59 17  6 59 46 24 34 17  7  6  0 15 12 24 51  6  6\n",
      "   6 17 34  4 57  6 45 12]]\n"
     ]
    }
   ],
   "source": [
    "print(input_batches[0][0:5])\n",
    "print(target_batches[0][0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 14.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sample(n=30, init_char=' '):\n",
    "    s = \"\"\n",
    "    hidden = model.init_hidden(batch_size=1)\n",
    "    input = Tensor(np.array([word2index[init_char]]))\n",
    "    for i in range(n):\n",
    "        rnn_input = embed.forward(input)\n",
    "        output, hidden = model.forward(input=rnn_input, hidden=hidden)\n",
    "        output.data *= 10 # temperature for sampling, higher=greedier\n",
    "        temp_dist = output.softmax()\n",
    "        temp_dist = temp_dist / temp_dist.sum()\n",
    "        #m = (temp_dist > np.random.rand()).argmax() # sample from pred\n",
    "        m = output.data.argmax() # take max of predictio\n",
    "        c = vocab[m]\n",
    "        input = Tensor(np.array([m]))\n",
    "        s += c\n",
    "    return s\n",
    "\n",
    "def train(iterations=100):\n",
    "    for iter in range(iterations):\n",
    "        total_loss = 0\n",
    "        n_loss = 0\n",
    "        hidden = model.init_hidden(batch_size=batch_size)\n",
    "        for batch_i in range(len(input_batches)):\n",
    "            hidden = Tensor(hidden.data, autograd=True)\n",
    "            loss = None\n",
    "            losses = list()\n",
    "            for t in range(bptt):\n",
    "                input = Tensor(input_batches[batch_i][t], autograd=True)\n",
    "                rnn_input = embed.forward(input=input)\n",
    "                output, hidden = model.forward(input=rnn_input, hidden=hidden)\n",
    "                \n",
    "                target = Tensor(target_batches[batch_i][t], autograd=True)\n",
    "                batch_loss = criterion.forward(output, target)\n",
    "                losses.append(batch_loss)\n",
    "                if(t == 0):\n",
    "                    loss = batch_loss\n",
    "                else:\n",
    "                    loss = loss + batch_loss\n",
    "\n",
    "            for loss in losses:\n",
    "                \"\"\n",
    "                    \n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "            total_loss += loss.data\n",
    "            log = \"\\r Iter:\" + str(iter)\n",
    "            log += \" - Batch \"+str(batch_i+1)+\"/\"+str(len(input_batches))\n",
    "            log += \" - Loss:\" + str(np.exp(total_loss / (batch_i+1)))\n",
    "            if(batch_i == 0):\n",
    "                log += \" - \" + generate_sample(70,'\\n').replace(\"\\n\",\" \")\n",
    "            if(batch_i % 10 == 0 or batch_i-1 == len(input_batches)):\n",
    "                sys.stdout.write(log)\n",
    "        optim.alpha *= 0.99\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Iter:0 - Batch 191/195 - Loss:90.449239911586244                                                                      \n",
      " Iter:1 - Batch 191/195 - Loss:20.894818315295982 a  th thtt thett tt tt tt tt tt tt tt tt tt tt tt tt tt tt tt tt tt tt\n",
      " Iter:2 - Batch 191/195 - Loss:15.613584632136503 a the the the the the the the the the the the the the the the the the \n",
      " Iter:3 - Batch 191/195 - Loss:13.226728456720428d tot the the the the the the the the the the the the the the the the \n",
      " Iter:4 - Batch 191/195 - Loss:11.892883950508265dh thert the the the the the the the the the the the the the the the t\n",
      " Iter:5 - Batch 191/195 - Loss:11.026551482998261dh thert the the the the the the the the the the the the the the the t\n",
      " Iter:6 - Batch 191/195 - Loss:10.374155134352871dh the the the the the the the the the the the the the the the the the\n",
      " Iter:7 - Batch 191/195 - Loss:9.8484359957554789dath the the the the the the the the the the the the the the the the t\n",
      " Iter:8 - Batch 191/195 - Loss:9.394899146964605 dath the the the the the the the the the the the the the the the the t\n",
      " Iter:9 - Batch 191/195 - Loss:8.997453915914774ther the the the the the the the the the the the the the the the the t\n",
      " Iter:10 - Batch 191/195 - Loss:8.648205290840892- ther the the the the the the the the the the the the the the the the t\n",
      " Iter:11 - Batch 191/195 - Loss:8.333827709374203 ther the the the the the the the the the the the the the the the the t\n",
      " Iter:12 - Batch 191/195 - Loss:8.051242209496241 the the the the the the the the the the the the the the the the the th\n",
      " Iter:13 - Batch 191/195 - Loss:7.7904405126864955the the the the the the the the the the the the the the the the the th\n",
      " Iter:14 - Batch 191/195 - Loss:7.5456789934023235 to the the the the the the the the the the the the the the the the the\n",
      " Iter:15 - Batch 191/195 - Loss:7.3144329030435785to the the the the the the the the the the the the the the the the the\n",
      " Iter:16 - Batch 191/195 - Loss:7.095521164411322to the the the the the the the the the the the the the the the the the\n",
      " Iter:17 - Batch 191/195 - Loss:6.8871188850015885 to the the the the the the the the the the the the the the the the the\n",
      " Iter:18 - Batch 191/195 - Loss:6.6866016797956755 to the the the the the the the the the the the the the the the the the\n",
      " Iter:19 - Batch 191/195 - Loss:6.4916244909683715to the the the the the the the the the the the the the the the the the\n",
      " Iter:20 - Batch 191/195 - Loss:6.300586488368573 to the the the the the the the the the the the the the the the the the\n",
      " Iter:21 - Batch 191/195 - Loss:6.1124013387542015 to the the the the the the the the the the the the the the the the the\n",
      " Iter:22 - Batch 191/195 - Loss:5.927074703035845 to the the the the the the the the the the the the the the the the the\n",
      " Iter:23 - Batch 191/195 - Loss:5.744703508600928o the the the the the the the the the the the the the the the the the\n",
      " Iter:24 - Batch 191/195 - Loss:5.5645986224289254to the the the the the the the the the the the the the the the the the\n",
      " Iter:25 - Batch 191/195 - Loss:5.386395625569219 to the to the to the to the to the to the to the to the to the to the \n",
      " Iter:26 - Batch 191/195 - Loss:5.210254296904865 to the to the to the to the to the to the to the to the to the to the \n",
      " Iter:27 - Batch 191/195 - Loss:5.0363786713283675to the wast the to the to the to the to the to the to the to the to th\n",
      " Iter:28 - Batch 191/195 - Loss:4.864627860720381 to the wast the to the to the to the to the to the to the to the to th\n",
      " Iter:29 - Batch 191/195 - Loss:4.6942397827381615to the wast the to the wast the to the wast the to the wast the to the\n",
      " Iter:30 - Batch 191/195 - Loss:4.5251801384901915to the wast the to the wast the to the wast the to the wast the to the\n",
      " Iter:31 - Batch 191/195 - Loss:4.3596086633053745post the wast the to the wast the to the wast the to the wast the to t\n",
      " Iter:32 - Batch 191/195 - Loss:4.1997714640240055I with to the werphink the wast the to the werphink the wast the to th\n",
      " Iter:33 - Batch 191/195 - Loss:4.045468093266275 I him, the to the to the to the to the to the to the to the to the to \n",
      " Iter:34 - Batch 191/195 - Loss:3.8957658040257873I himst to the werphink the to the werphink the to the werphink the to\n",
      " Iter:35 - Batch 191/195 - Loss:3.7499696555240885I himm the to the werphink the to the werphink the to the werphink the\n",
      " Iter:36 - Batch 191/195 - Loss:3.6086397270074713 I himm the wet, sirind hims, and the to the wet, sirind hims, and the \n",
      " Iter:37 - Batch 191/195 - Loss:3.4714540770182203I himm the wet, sirind hims, and the wet, sirind hims, and the wet, si\n",
      " Iter:38 - Batch 191/195 - Loss:3.3387485872650246I himm the wast the worsest to the wet, sirind hims, and the to the we\n",
      " Iter:39 - Batch 191/195 - Loss:3.2106455262817826I himm the tamn to the wet, siritherd the tamn to the wet, siritherd t\n",
      " Iter:40 - Batch 191/195 - Loss:3.0860189551343784blum the tamn to the worsest and the tamn to the worsest and the tamn \n",
      " Iter:41 - Batch 191/195 - Loss:2.9646736607676976lum the tamn to the tamn to the tamn to the tamn to the tamn to the t\n",
      " Iter:42 - Batch 191/195 - Loss:2.8485216477936386 blum the tamn to the tamn to the tamn to the tamn to the tamn to the t\n",
      " Iter:43 - Batch 191/195 - Loss:2.7369767506673517blum the tamn the tamn the tamn the tamn the tamn the tamn the tamn th\n",
      " Iter:44 - Batch 191/195 - Loss:2.6274607087442376 blum the tamn to the tamn the tamn to the tamn the tamn to the tamn th\n",
      " Iter:45 - Batch 191/195 - Loss:2.5229171986510623 blum the tamn to the tamn to the tamn to the tamn to the tamn to the t\n",
      " Iter:46 - Batch 191/195 - Loss:2.4222361442275546 blum the tamn to the tamn to the tamn to the tamn to the tamn to the t\n",
      " Iter:47 - Batch 191/195 - Loss:2.3251558709547254 blum the mant to the prom the mant to the prom the mant to the prom th\n",
      " Iter:48 - Batch 191/195 - Loss:2.2324215182916013blum the mant to the pringmen to the pringmen to the pringmen to the p\n",
      " Iter:49 - Batch 191/195 - Loss:2.1490147628720754blum the shallanday and she shallanday and she shallanday and she shal\n",
      " Iter:50 - Batch 191/195 - Loss:2.0655302664328627 blum the shallanday and she shallanday and she shallanday and she shal\n",
      " Iter:51 - Batch 191/195 - Loss:1.9882400083191638 blum the shallanday and she shallanday and she shallanday and she shal\n",
      " Iter:52 - Batch 191/195 - Loss:1.9107033261904898 blum the shallanday and she shallanday and she shallanday and she shal\n",
      " Iter:53 - Batch 191/195 - Loss:1.8380780418231158 blum the shallanday and she shallanday and she shallanday and she shal\n",
      " Iter:54 - Batch 191/195 - Loss:1.7739752512007596 blum the nimm.  CHESSINI Ant I was nammet to the stake the night to th\n",
      " Iter:55 - Batch 191/195 - Loss:1.7138768798843094 blum this speak: Hishallandake the night to the nimm.  CHESSINI And sh\n",
      " Iter:56 - Batch 191/195 - Loss:1.6584183417698608 blum this stol my sirth, and say wast this to the night this hady am t\n",
      " Iter:57 - Batch 191/195 - Loss:1.6151741049644817blum this begget to the night to the mands, and she shallandaknto'd hi\n",
      " Iter:58 - Batch 191/195 - Loss:1.5665420144678068 blum this speake the mands, and she prom the mands, and she prom the m\n",
      " Iter:59 - Batch 191/195 - Loss:1.5143132854845593 blum this begget to the night this be, prenthis to the night this be, \n",
      " Iter:60 - Batch 191/195 - Loss:1.4848968115774814 blum this begget to the namands, and say word to the mands, and say wo\n",
      " Iter:61 - Batch 191/195 - Loss:1.4326225081469537blum this bachimy faceive seemperfessardins, and himmands, and say wou\n",
      " Iter:62 - Batch 191/195 - Loss:1.4305572432022116 blum this speakither buins, balt shallandaknto the namands, and say wo\n",
      " Iter:63 - Batch 191/195 - Loss:1.3773304364928751 blum this speakithere wot camit speakithere wot camit speakithere wot \n",
      " Iter:64 - Batch 191/195 - Loss:1.3336826430364366 I have the name, and say word this stol my hear are entreakes, to the \n",
      " Iter:65 - Batch 191/195 - Loss:1.3032026592744863 blum your jandakithe nammalt from the mands, and say wout kear are, th\n",
      " Iter:66 - Batch 191/195 - Loss:1.2856178857390352 I am my she'l him such him, to the mands, and say wout kel, Be struck \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Iter:67 - Batch 191/195 - Loss:1.2636609041260494 I have the nammalt fellow, Camy hot comporth, ard, and she with eyed h\n",
      " Iter:68 - Batch 191/195 - Loss:1.2388008479948125I am my hear ared himmst Lo my fachain firt, in hemphim that I will si\n",
      " Iter:69 - Batch 191/195 - Loss:1.2308689428653058 I have the nabe I nammath, and say would nothere it is, amather but ma\n",
      " Iter:70 - Batch 191/195 - Loss:1.2190525630831861 I have them this sto But ke That ever have stringmen I man!  HENRE: Th\n",
      " Iter:71 - Batch 191/195 - Loss:1.1952278926978983 am thy tamn the mand?  ARGLAPlook sonithou this sto But ke That ever\n",
      " Iter:72 - Batch 191/195 - Loss:1.2030174664801154 I am the mant what makes: She waser, sir, now, but I will suchims ar l\n",
      " Iter:73 - Batch 191/195 - Loss:1.1855843008832223 I am the mand?  ARGLAPlET: YORI himy hath ser, my hot and I with ey wa\n",
      " Iter:74 - Batch 191/195 - Loss:1.1845803311950502 I am the mant wot in the profets of thy tamn the mand?  ARGLAPlook son\n",
      " Iter:75 - Batch 191/195 - Loss:1.1817663089306305 blum shains bachakes: Hot Ramingmen I man!  HENREY: Therefore you chas\n",
      " Iter:76 - Batch 191/195 - Loss:1.1408842510244204I am the mand?  ARGLAPlET: YORI her, Which thuck Why, speakes Mid have\n",
      " Iter:77 - Batch 191/195 - Loss:1.1247152733453305 I am thy tamnto ht this sto But ke That ever have shall him this sto B\n",
      " Iter:78 - Batch 191/195 - Loss:1.1165593872198258blum his grace shains, amak to the mant what makes: Hot upon wast bish\n",
      " Iter:79 - Batch 191/195 - Loss:1.1116526090393313 I am the mand?  ARt Lam'd her hims, amak to the nace shainted mitherd \n",
      " Iter:80 - Batch 191/195 - Loss:1.1666700282202774 blum That man'arkandingment, the mant wot in makntreaknto't the mant w\n",
      " Iter:81 - Batch 191/195 - Loss:1.1165591877197452I am the mant wance, of thy his grace shain ke.  CHELO: And shainted y\n",
      " Iter:82 - Batch 191/195 - Loss:1.0981291030546974 I am muy if me this stold Befrady to the nace shain fither hims, with \n",
      " Iter:83 - Batch 191/195 - Loss:1.1032515533560994 I am mand?  ARGLATlAT: Yet diff begainted outs affacthis tas common: a\n",
      " Iter:84 - Batch 191/195 - Loss:1.2311558421388107I all that I will ligget'n the mands are, this stold the niand, Whenis\n",
      " Iter:85 - Batch 191/195 - Loss:1.1681472700550806 I have offentreakes: Sour had suarfurth.  CENSI have offentreakes: Sou\n",
      " Iter:86 - Batch 191/195 - Loss:1.1012707204029581 blum his grace shain fitherd eself the mand himine him, miveralth, his\n",
      " Iter:87 - Batch 191/195 - Loss:1.0794503898312362 I all that I will light for my faceing from the mand him, mimm the man\n",
      " Iter:88 - Batch 191/195 - Loss:1.0700392572364559 I all that I will light the tamn the mand him, mimm the mand him, mimm\n",
      " Iter:89 - Batch 191/195 - Loss:1.0633497052772318 I all that I will light for my phall sir, no my camphimsed Poises not \n",
      " Iter:90 - Batch 191/195 - Loss:1.0627818955582653 I all that I will lights, Whou a will be goth hould not?  hememness of\n",
      " Iter:91 - Batch 191/195 - Loss:1.0560498744221012 all that I will lights, Whou a will be goth hould bight the pardine \n",
      " Iter:92 - Batch 191/195 - Loss:1.0515261077903639 I all that I will light for the mand him, mimm the mand him, mimm the \n",
      " Iter:93 - Batch 191/195 - Loss:1.0474624293455483 I all that I will light for the mand him, miveral sweepingment, told B\n",
      " Iter:94 - Batch 191/195 - Loss:1.0446612298713484 I all that I will light for the mand him, my face.  TOVE: By go, sir, \n",
      " Iter:95 - Batch 191/195 - Loss:1.0425213678956557I all that I will light for the nace she you he would bight the pardin\n",
      " Iter:96 - Batch 191/195 - Loss:1.0415432960206196 I all that I will light for the nace she you he worses to their he loo\n",
      " Iter:97 - Batch 191/195 - Loss:1.0390637220299117 I all that I will light for the nace she you he worses to their he loo\n",
      " Iter:98 - Batch 191/195 - Loss:1.0375564234358432 I all that I will light for the nace she you he would brod man, have n\n",
      " Iter:99 - Batch 191/195 - Loss:1.0357311828591735 I all that I will light for the nace she you he worses to pardins, ama\n"
     ]
    }
   ],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 14.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I all that I will light for the nace she you he would brod man, have neave name, and I conow, to ther\n",
      "But shakes:\n",
      "Seak:\n",
      "That I will light for the nace she you he would brod man, have neave name, and I conow, to ther\n",
      "But shakes:\n",
      "Seak:\n",
      "That I will light for the nace she you he would brod man, have neave name, and I conow, to ther\n",
      "But shakes:\n",
      "Seak:\n",
      "That I will light for the nace she you he would brod man, have neave name, and I conow, to ther\n",
      "But shakes:\n",
      "Seak:\n",
      "That I will light for the nace she you he would brod man, have neave name, and I conow, to ther\n",
      "But shakes:\n",
      "Seak:\n",
      "That I will light for the nace she you he would brod man, have neave name, and I conow, to ther\n",
      "But shakes:\n",
      "Seak:\n",
      "That I will light for the nace she you he would brod man, have neave name, and I conow, to ther\n",
      "But shakes:\n",
      "Seak:\n",
      "That I will light for the nace she you he would brod man, have neave name, and I conow, to ther\n",
      "But shakes:\n",
      "Seak:\n",
      "That I will light for the nace she you he would brod man, have neave name, and I conow, to ther\n",
      "But shakes:\n",
      "Seak:\n",
      "That I will light for the nace she you he would brod man, have neave name, and I conow, to ther\n",
      "But shakes:\n",
      "Seak:\n",
      "That I will light for the nace she you he would brod man, have neave name, and I conow, to ther\n",
      "But shakes:\n",
      "Seak:\n",
      "That I will light for the nace she you he would brod man, have neave name, and I conow, to ther\n",
      "But shakes:\n",
      "Seak:\n",
      "That I will light for the nace she you he would brod man, have neave name, and I conow, to ther\n",
      "But shakes:\n",
      "Seak:\n",
      "That I will light for the nace she you he would brod man, have neave name, and I conow, to ther\n",
      "But shakes:\n",
      "Seak:\n",
      "That I will light for the nace she you he would brod man, have neave name, and I conow, to ther\n",
      "But shakes:\n",
      "Seak:\n",
      "That I will light for the nace she you he would brod man, have neave name, and I conow, to ther\n",
      "But shakes:\n",
      "Seak:\n",
      "That I will light for the nace she you he would brod man, have neave name, and I conow, to ther\n",
      "But shakes:\n",
      "Seak:\n",
      "That I will light for the nace she you he would brod man\n"
     ]
    }
   ],
   "source": [
    "print(generate_sample(n=2000, init_char='\\n'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 14.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sigmoid Activations\n",
      "[0.93940638 0.96852968]\n",
      "[0.9919462  0.99121735]\n",
      "[0.99301385 0.99302901]\n",
      "[0.9930713  0.99307098]\n",
      "[0.99307285 0.99307285]\n",
      "[0.99307291 0.99307291]\n",
      "[0.99307291 0.99307291]\n",
      "[0.99307291 0.99307291]\n",
      "[0.99307291 0.99307291]\n",
      "[0.99307291 0.99307291]\n",
      "\n",
      "Sigmoid Gradients\n",
      "[0.03439552 0.03439552]\n",
      "[0.00118305 0.00118305]\n",
      "[4.06916726e-05 4.06916726e-05]\n",
      "[1.39961115e-06 1.39961115e-06]\n",
      "[4.81403643e-08 4.81403637e-08]\n",
      "[1.65582672e-09 1.65582765e-09]\n",
      "[5.69682675e-11 5.69667160e-11]\n",
      "[1.97259346e-12 1.97517920e-12]\n",
      "[8.45387597e-14 8.02306381e-14]\n",
      "[1.45938177e-14 2.16938983e-14]\n",
      "Activations\n",
      "[4.8135251  4.72615519]\n",
      "[23.71814585 23.98025559]\n",
      "[119.63916823 118.852839  ]\n",
      "[595.05052421 597.40951192]\n",
      "[2984.68857188 2977.61160877]\n",
      "[14895.13500696 14916.36589628]\n",
      "[74560.59859209 74496.90592414]\n",
      "[372548.22228863 372739.30029248]\n",
      "[1863505.42345854 1862932.18944699]\n",
      "[9315234.18124649 9316953.88328115]\n",
      "\n",
      "Gradients\n",
      "[5. 5.]\n",
      "[25. 25.]\n",
      "[125. 125.]\n",
      "[625. 625.]\n",
      "[3125. 3125.]\n",
      "[15625. 15625.]\n",
      "[78125. 78125.]\n",
      "[390625. 390625.]\n",
      "[1953125. 1953125.]\n",
      "[9765625. 9765625.]\n"
     ]
    }
   ],
   "source": [
    "(sigmoid,relu)=(lambda x:1/(1+np.exp(-x)), lambda x:(x>0).astype(float)*x)\n",
    "weights = np.array([[1,4],[4,1]])\n",
    "activation = sigmoid(np.array([1,0.01]))\n",
    "print(\"Sigmoid Activations\")\n",
    "activations = list()\n",
    "for iter in range(10):\n",
    "    activation = sigmoid(activation.dot(weights))\n",
    "    activations.append(activation)\n",
    "    print(activation)\n",
    "\n",
    "print(\"\\nSigmoid Gradients\")\n",
    "gradient = np.ones_like(activation)\n",
    "for activation in reversed(activations):\n",
    "    gradient = (activation * (1 - activation) * gradient)\n",
    "    gradient = gradient.dot(weights.transpose())\n",
    "    print(gradient)\n",
    "\n",
    "print(\"Activations\")\n",
    "activations = list()\n",
    "for iter in range(10):\n",
    "    activation = relu(activation.dot(weights))\n",
    "    activations.append(activation)\n",
    "    print(activation)\n",
    "\n",
    "print(\"\\nGradients\")\n",
    "gradient = np.ones_like(activation)\n",
    "for activation in reversed(activations):\n",
    "    gradient = ((activation > 0) * gradient).dot(weights.transpose())\n",
    "    print(gradient)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 14.10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMCell(Layer):\n",
    "    def __init__(self, n_inputs, n_hidden, n_output):\n",
    "        super().__init__()\n",
    "        self.n_inputs = n_inputs\n",
    "        self.n_hidden = n_hidden\n",
    "        self.n_output = n_output\n",
    "        self.xf = Linear(n_inputs, n_hidden)\n",
    "        self.xi = Linear(n_inputs, n_hidden)\n",
    "        self.xo = Linear(n_inputs, n_hidden)\n",
    "        self.xc = Linear(n_inputs, n_hidden)\n",
    "        self.hf = Linear(n_hidden, n_hidden, bias=False)\n",
    "        self.hi = Linear(n_hidden, n_hidden, bias=False)\n",
    "        self.ho = Linear(n_hidden, n_hidden, bias=False)\n",
    "        self.hc = Linear(n_hidden, n_hidden, bias=False)\n",
    "        self.w_ho = Linear(n_hidden, n_output, bias=False)\n",
    "        self.parameters += self.xf.get_parameters()\n",
    "        self.parameters += self.xi.get_parameters()\n",
    "        self.parameters += self.xo.get_parameters()\n",
    "        self.parameters += self.xc.get_parameters()\n",
    "        self.parameters += self.hf.get_parameters()\n",
    "        self.parameters += self.hi.get_parameters()\n",
    "        self.parameters += self.ho.get_parameters()\n",
    "        self.parameters += self.hc.get_parameters()\n",
    "        self.parameters += self.w_ho.get_parameters()\n",
    "    def forward(self, input, hidden):\n",
    "        prev_hidden = hidden[0]\n",
    "        prev_cell = hidden[1]\n",
    "        f=(self.xf.forward(input)+self.hf.forward(prev_hidden)).sigmoid()\n",
    "        i=(self.xi.forward(input)+self.hi.forward(prev_hidden)).sigmoid()\n",
    "        o=(self.xo.forward(input)+self.ho.forward(prev_hidden)).sigmoid()\n",
    "        g = (self.xc.forward(input) +self.hc.forward(prev_hidden)).tanh()\n",
    "        c = (f * prev_cell) + (i * g)\n",
    "        h = o * c.tanh()\n",
    "        h.is_recurrent = True\n",
    "        output = self.w_ho.forward(h)\n",
    "        return output, (h, c)\n",
    "    def init_hidden(self, batch_size=1):\n",
    "        h = Tensor(np.zeros((batch_size, self.n_hidden)), autograd=True)\n",
    "        c = Tensor(np.zeros((batch_size, self.n_hidden)), autograd=True)\n",
    "        h.data[:,0] += 1\n",
    "        c.data[:,0] += 1\n",
    "        return (h, c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 14.11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys,random,math\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import sys\n",
    "np.random.seed(0)\n",
    "\n",
    "f = open('datasets/shakespear.txt','r')\n",
    "raw = f.read()\n",
    "f.close()\n",
    "vocab = list(set(raw))\n",
    "word2index = {}\n",
    "for i,word in enumerate(vocab):\n",
    "    word2index[word]=i\n",
    "indices = np.array(list(map(lambda x:word2index[x], raw)))\n",
    "embed = Embedding(vocab_size=len(vocab),dim=512)\n",
    "model = LSTMCell(n_inputs=512, n_hidden=512, n_output=len(vocab))\n",
    "model.w_ho.weight.data *= 0 # this seemed to help training\n",
    "criterion = CrossEntropyLoss()\n",
    "optim = SGD(parameters=model.get_parameters() + embed.get_parameters(),alpha=0.05)\n",
    "batch_size = 16\n",
    "bptt = 25\n",
    "n_batches = int((indices.shape[0] / (batch_size)))\n",
    "trimmed_indices = indices[:n_batches*batch_size]\n",
    "batched_indices = trimmed_indices.reshape(batch_size, n_batches)\n",
    "batched_indices = batched_indices.transpose()\n",
    "input_batched_indices = batched_indices[0:-1]\n",
    "target_batched_indices = batched_indices[1:]\n",
    "n_bptt = int(((n_batches-1) / bptt))\n",
    "input_batches = input_batched_indices[:n_bptt*bptt]\n",
    "input_batches = input_batches.reshape(n_bptt,bptt,batch_size)\n",
    "target_batches = target_batched_indices[:n_bptt*bptt]\n",
    "target_batches = target_batches.reshape(n_bptt, bptt, batch_size)\n",
    "min_loss = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 14.12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sample(n=30, init_char=' '):\n",
    "    s = \"\"\n",
    "    hidden = model.init_hidden(batch_size=1)\n",
    "    input = Tensor(np.array([word2index[init_char]]))\n",
    "    for i in range(n):\n",
    "        rnn_input = embed.forward(input)\n",
    "        output, hidden = model.forward(input=rnn_input, hidden=hidden)\n",
    "        output.data *= 15\n",
    "        temp_dist = output.softmax()\n",
    "        temp_dist /= temp_dist.sum()\n",
    "        # m = (temp_dist > np.random.rand()).argmax() # sample from pred\n",
    "        m = output.data.argmax() # take the max prediction\n",
    "        c = vocab[m]\n",
    "        input = Tensor(np.array([m]))\n",
    "        s += c\n",
    "    return s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(iterations=100, min_loss=1000):\n",
    "    for iter in range(iterations):\n",
    "        total_loss, n_loss = (0, 0)\n",
    "        hidden = model.init_hidden(batch_size=batch_size)\n",
    "        batches_to_train = len(input_batches)\n",
    "        for batch_i in range(batches_to_train):\n",
    "            hidden = (Tensor(hidden[0].data, autograd=True), Tensor(hidden[1].data, autograd=True))\n",
    "            losses = list()\n",
    "            for t in range(bptt):\n",
    "                input = Tensor(input_batches[batch_i][t], autograd=True)\n",
    "                rnn_input = embed.forward(input=input)\n",
    "                output, hidden = model.forward(input=rnn_input, hidden=hidden)\n",
    "                target = Tensor(target_batches[batch_i][t], autograd=True)\n",
    "                batch_loss = criterion.forward(output, target)\n",
    "                if(t == 0):\n",
    "                    losses.append(batch_loss)\n",
    "                else:\n",
    "                    losses.append(batch_loss + losses[-1])\n",
    "            loss = losses[-1]\n",
    "\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "            total_loss += loss.data / bptt\n",
    "            epoch_loss = np.exp(total_loss / (batch_i+1))\n",
    "            if(epoch_loss < min_loss):\n",
    "                min_loss = epoch_loss\n",
    "                print()\n",
    "\n",
    "            log = \"\\r Iter:\" + str(iter)\n",
    "            log += \" - Alpha:\" + str(optim.alpha)[0:5]\n",
    "            log += \" - Batch \"+str(batch_i+1)+\"/\"+str(len(input_batches))\n",
    "            log += \" - Min Loss:\" + str(min_loss)[0:5]\n",
    "            log += \" - Loss:\" + str(epoch_loss)\n",
    "            if(batch_i % 50 == 0):\n",
    "                s = generate_sample(n=70, init_char='T').replace(\"\\n\",\" \")\n",
    "                log += \" - \" + s\n",
    "            sys.stdout.write(log)\n",
    "\n",
    "        optim.alpha *= 0.99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Iter:0 - Alpha:0.05 - Batch 1/249 - Min Loss:62.00 - Loss:62.000000000000064 -                      eeee  eeee  eeee  eee  eeee  eee  eee  eee  eeee \n",
      " Iter:0 - Alpha:0.05 - Batch 2/249 - Min Loss:61.99 - Loss:61.999195513086285\n",
      " Iter:0 - Alpha:0.05 - Batch 3/249 - Min Loss:61.98 - Loss:61.984910016040885\n",
      " Iter:0 - Alpha:0.05 - Batch 4/249 - Min Loss:61.95 - Loss:61.95759317523022\n",
      " Iter:0 - Alpha:0.05 - Batch 5/249 - Min Loss:61.90 - Loss:61.90392927051005\n",
      " Iter:0 - Alpha:0.05 - Batch 6/249 - Min Loss:61.79 - Loss:61.79490938754441\n",
      " Iter:0 - Alpha:0.05 - Batch 7/249 - Min Loss:61.60 - Loss:61.60023379161083\n",
      " Iter:0 - Alpha:0.05 - Batch 8/249 - Min Loss:61.11 - Loss:61.1156884335555\n",
      " Iter:0 - Alpha:0.05 - Batch 9/249 - Min Loss:60.06 - Loss:60.069593271418235\n",
      " Iter:0 - Alpha:0.05 - Batch 10/249 - Min Loss:58.60 - Loss:58.60326515002265\n",
      " Iter:0 - Alpha:0.05 - Batch 11/249 - Min Loss:56.11 - Loss:56.11448262460074\n",
      " Iter:0 - Alpha:0.05 - Batch 12/249 - Min Loss:53.88 - Loss:53.887629980824634\n",
      " Iter:0 - Alpha:0.05 - Batch 13/249 - Min Loss:51.35 - Loss:51.358031544413244\n",
      " Iter:0 - Alpha:0.05 - Batch 14/249 - Min Loss:50.40 - Loss:50.40485088049126\n",
      " Iter:0 - Alpha:0.05 - Batch 15/249 - Min Loss:49.43 - Loss:49.43832365225467\n",
      " Iter:0 - Alpha:0.05 - Batch 16/249 - Min Loss:48.05 - Loss:48.05925387664802\n",
      " Iter:0 - Alpha:0.05 - Batch 17/249 - Min Loss:46.41 - Loss:46.41953394990056\n",
      " Iter:0 - Alpha:0.05 - Batch 18/249 - Min Loss:45.25 - Loss:45.255805454015835\n",
      " Iter:0 - Alpha:0.05 - Batch 19/249 - Min Loss:44.14 - Loss:44.14618966596847\n",
      " Iter:0 - Alpha:0.05 - Batch 20/249 - Min Loss:43.24 - Loss:43.248798610990725\n",
      " Iter:0 - Alpha:0.05 - Batch 21/249 - Min Loss:42.63 - Loss:42.638420668725466\n",
      " Iter:0 - Alpha:0.05 - Batch 22/249 - Min Loss:42.09 - Loss:42.09587506649989\n",
      " Iter:0 - Alpha:0.05 - Batch 23/249 - Min Loss:41.45 - Loss:41.45468284382924\n",
      " Iter:0 - Alpha:0.05 - Batch 24/249 - Min Loss:40.50 - Loss:40.506129501809276\n",
      " Iter:0 - Alpha:0.05 - Batch 25/249 - Min Loss:39.72 - Loss:39.72435574676283\n",
      " Iter:0 - Alpha:0.05 - Batch 26/249 - Min Loss:38.92 - Loss:38.92672696556255\n",
      " Iter:0 - Alpha:0.05 - Batch 27/249 - Min Loss:38.59 - Loss:38.590003035990605\n",
      " Iter:0 - Alpha:0.05 - Batch 28/249 - Min Loss:37.96 - Loss:37.964212158131964\n",
      " Iter:0 - Alpha:0.05 - Batch 29/249 - Min Loss:37.54 - Loss:37.54468298622297\n",
      " Iter:0 - Alpha:0.05 - Batch 30/249 - Min Loss:37.23 - Loss:37.23433343066644\n",
      " Iter:0 - Alpha:0.05 - Batch 31/249 - Min Loss:36.91 - Loss:36.913317800010454\n",
      " Iter:0 - Alpha:0.05 - Batch 32/249 - Min Loss:36.55 - Loss:36.55137003021443\n",
      " Iter:0 - Alpha:0.05 - Batch 33/249 - Min Loss:36.18 - Loss:36.18754535606371\n",
      " Iter:0 - Alpha:0.05 - Batch 34/249 - Min Loss:35.87 - Loss:35.877114045145014\n",
      " Iter:0 - Alpha:0.05 - Batch 35/249 - Min Loss:35.67 - Loss:35.67201117245996\n",
      " Iter:0 - Alpha:0.05 - Batch 36/249 - Min Loss:35.47 - Loss:35.47055244735085\n",
      " Iter:0 - Alpha:0.05 - Batch 37/249 - Min Loss:35.18 - Loss:35.18112252252527\n",
      " Iter:0 - Alpha:0.05 - Batch 38/249 - Min Loss:34.95 - Loss:34.95494994035479\n",
      " Iter:0 - Alpha:0.05 - Batch 39/249 - Min Loss:34.90 - Loss:34.902954370957964\n",
      " Iter:0 - Alpha:0.05 - Batch 40/249 - Min Loss:34.59 - Loss:34.59259280897612\n",
      " Iter:0 - Alpha:0.05 - Batch 42/249 - Min Loss:34.50 - Loss:34.55339870914906\n",
      " Iter:0 - Alpha:0.05 - Batch 43/249 - Min Loss:34.28 - Loss:34.28675357757005\n",
      " Iter:0 - Alpha:0.05 - Batch 44/249 - Min Loss:34.05 - Loss:34.05739157577799\n",
      " Iter:0 - Alpha:0.05 - Batch 45/249 - Min Loss:33.92 - Loss:33.92964015984888\n",
      " Iter:0 - Alpha:0.05 - Batch 46/249 - Min Loss:33.71 - Loss:33.71659386635174\n",
      " Iter:0 - Alpha:0.05 - Batch 47/249 - Min Loss:33.52 - Loss:33.524420140598814\n",
      " Iter:0 - Alpha:0.05 - Batch 48/249 - Min Loss:33.32 - Loss:33.32065222514824\n",
      " Iter:0 - Alpha:0.05 - Batch 49/249 - Min Loss:33.17 - Loss:33.176114142571656\n",
      " Iter:0 - Alpha:0.05 - Batch 50/249 - Min Loss:32.99 - Loss:32.99322972308527\n",
      " Iter:0 - Alpha:0.05 - Batch 51/249 - Min Loss:32.71 - Loss:32.71011863480819 -  t e t e e t e t e t e t e t e t e t e t e t e t e t e t e t e t e t e\n",
      " Iter:0 - Alpha:0.05 - Batch 52/249 - Min Loss:32.50 - Loss:32.504772451496734\n",
      " Iter:0 - Alpha:0.05 - Batch 53/249 - Min Loss:32.30 - Loss:32.30566972142884\n",
      " Iter:0 - Alpha:0.05 - Batch 54/249 - Min Loss:32.13 - Loss:32.136683378447344\n",
      " Iter:0 - Alpha:0.05 - Batch 55/249 - Min Loss:31.95 - Loss:31.952322639023425\n",
      " Iter:0 - Alpha:0.05 - Batch 56/249 - Min Loss:31.78 - Loss:31.78682176235428\n",
      " Iter:0 - Alpha:0.05 - Batch 57/249 - Min Loss:31.57 - Loss:31.57293175821498\n",
      " Iter:0 - Alpha:0.05 - Batch 58/249 - Min Loss:31.40 - Loss:31.407102460004896\n",
      " Iter:0 - Alpha:0.05 - Batch 59/249 - Min Loss:31.21 - Loss:31.213503552015585\n",
      " Iter:0 - Alpha:0.05 - Batch 60/249 - Min Loss:31.04 - Loss:31.04291716645753\n",
      " Iter:0 - Alpha:0.05 - Batch 61/249 - Min Loss:30.82 - Loss:30.820711478744194\n",
      " Iter:0 - Alpha:0.05 - Batch 62/249 - Min Loss:30.64 - Loss:30.640949695854662\n",
      " Iter:0 - Alpha:0.05 - Batch 63/249 - Min Loss:30.41 - Loss:30.411254361079962\n",
      " Iter:0 - Alpha:0.05 - Batch 64/249 - Min Loss:30.22 - Loss:30.229426302193865\n",
      " Iter:0 - Alpha:0.05 - Batch 66/249 - Min Loss:30.21 - Loss:30.249396308690237\n",
      " Iter:0 - Alpha:0.05 - Batch 67/249 - Min Loss:30.14 - Loss:30.144926553165003\n",
      " Iter:0 - Alpha:0.05 - Batch 68/249 - Min Loss:29.92 - Loss:29.92493734408607\n",
      " Iter:0 - Alpha:0.05 - Batch 69/249 - Min Loss:29.81 - Loss:29.815319775173005\n",
      " Iter:0 - Alpha:0.05 - Batch 70/249 - Min Loss:29.65 - Loss:29.65039143823397\n",
      " Iter:0 - Alpha:0.05 - Batch 71/249 - Min Loss:29.55 - Loss:29.554553011857717\n",
      " Iter:0 - Alpha:0.05 - Batch 72/249 - Min Loss:29.51 - Loss:29.51726283480801\n",
      " Iter:0 - Alpha:0.05 - Batch 73/249 - Min Loss:29.41 - Loss:29.418172886516622\n",
      " Iter:0 - Alpha:0.05 - Batch 74/249 - Min Loss:29.24 - Loss:29.241692814486033\n",
      " Iter:0 - Alpha:0.05 - Batch 75/249 - Min Loss:29.11 - Loss:29.11531453713197\n",
      " Iter:0 - Alpha:0.05 - Batch 76/249 - Min Loss:29.02 - Loss:29.025167079103333\n",
      " Iter:0 - Alpha:0.05 - Batch 77/249 - Min Loss:28.89 - Loss:28.897850532223067\n",
      " Iter:0 - Alpha:0.05 - Batch 78/249 - Min Loss:28.82 - Loss:28.828846490513946\n",
      " Iter:0 - Alpha:0.05 - Batch 79/249 - Min Loss:28.70 - Loss:28.70826325545594\n",
      " Iter:0 - Alpha:0.05 - Batch 80/249 - Min Loss:28.59 - Loss:28.59092398123521\n",
      " Iter:0 - Alpha:0.05 - Batch 81/249 - Min Loss:28.43 - Loss:28.435513174510216\n",
      " Iter:0 - Alpha:0.05 - Batch 82/249 - Min Loss:28.35 - Loss:28.355717719464067\n",
      " Iter:0 - Alpha:0.05 - Batch 86/249 - Min Loss:28.29 - Loss:28.428169960199774\n",
      " Iter:0 - Alpha:0.05 - Batch 87/249 - Min Loss:28.27 - Loss:28.274264811667393\n",
      " Iter:0 - Alpha:0.05 - Batch 88/249 - Min Loss:28.12 - Loss:28.123919596498233\n",
      " Iter:0 - Alpha:0.05 - Batch 89/249 - Min Loss:27.97 - Loss:27.970178366076176\n",
      " Iter:0 - Alpha:0.05 - Batch 90/249 - Min Loss:27.83 - Loss:27.838829935046924\n",
      " Iter:0 - Alpha:0.05 - Batch 91/249 - Min Loss:27.72 - Loss:27.72476081640086\n",
      " Iter:0 - Alpha:0.05 - Batch 92/249 - Min Loss:27.63 - Loss:27.63054945754652\n",
      " Iter:0 - Alpha:0.05 - Batch 93/249 - Min Loss:27.48 - Loss:27.489593097532765\n",
      " Iter:0 - Alpha:0.05 - Batch 94/249 - Min Loss:27.36 - Loss:27.36544244603508\n",
      " Iter:0 - Alpha:0.05 - Batch 95/249 - Min Loss:27.23 - Loss:27.23016437750449\n",
      " Iter:0 - Alpha:0.05 - Batch 96/249 - Min Loss:27.13 - Loss:27.138481750433936\n",
      " Iter:0 - Alpha:0.05 - Batch 97/249 - Min Loss:27.02 - Loss:27.02610659096547\n",
      " Iter:0 - Alpha:0.05 - Batch 98/249 - Min Loss:26.91 - Loss:26.91658150945678\n",
      " Iter:0 - Alpha:0.05 - Batch 99/249 - Min Loss:26.75 - Loss:26.75927494967456\n",
      " Iter:0 - Alpha:0.05 - Batch 100/249 - Min Loss:26.62 - Loss:26.625710673111485\n",
      " Iter:0 - Alpha:0.05 - Batch 101/249 - Min Loss:26.49 - Loss:26.491630932971795 -  we th t te t te t te t te t te t te t te t te t te t te t te t te t t\n",
      " Iter:0 - Alpha:0.05 - Batch 102/249 - Min Loss:26.37 - Loss:26.37301927629116\n",
      " Iter:0 - Alpha:0.05 - Batch 103/249 - Min Loss:26.27 - Loss:26.2730575680946\n",
      " Iter:0 - Alpha:0.05 - Batch 104/249 - Min Loss:26.21 - Loss:26.21157456078586\n",
      " Iter:0 - Alpha:0.05 - Batch 105/249 - Min Loss:26.18 - Loss:26.18674351367922\n",
      " Iter:0 - Alpha:0.05 - Batch 106/249 - Min Loss:26.07 - Loss:26.072498036546914\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Iter:0 - Alpha:0.05 - Batch 107/249 - Min Loss:26.01 - Loss:26.01443369180936\n",
      " Iter:0 - Alpha:0.05 - Batch 108/249 - Min Loss:25.92 - Loss:25.923093969517172\n",
      " Iter:0 - Alpha:0.05 - Batch 109/249 - Min Loss:25.84 - Loss:25.840435036424903\n",
      " Iter:0 - Alpha:0.05 - Batch 110/249 - Min Loss:25.74 - Loss:25.74601448371527\n",
      " Iter:0 - Alpha:0.05 - Batch 111/249 - Min Loss:25.63 - Loss:25.635654470407317\n",
      " Iter:0 - Alpha:0.05 - Batch 112/249 - Min Loss:25.53 - Loss:25.539407537322294\n",
      " Iter:0 - Alpha:0.05 - Batch 113/249 - Min Loss:25.44 - Loss:25.44783482988351\n",
      " Iter:0 - Alpha:0.05 - Batch 114/249 - Min Loss:25.38 - Loss:25.387315404997516\n",
      " Iter:0 - Alpha:0.05 - Batch 115/249 - Min Loss:25.36 - Loss:25.36250574321817\n",
      " Iter:0 - Alpha:0.05 - Batch 116/249 - Min Loss:25.29 - Loss:25.29250471021678\n",
      " Iter:0 - Alpha:0.05 - Batch 117/249 - Min Loss:25.21 - Loss:25.21591590393694\n",
      " Iter:0 - Alpha:0.05 - Batch 118/249 - Min Loss:25.12 - Loss:25.120892852664728\n",
      " Iter:0 - Alpha:0.05 - Batch 119/249 - Min Loss:25.05 - Loss:25.051117938627588\n",
      " Iter:0 - Alpha:0.05 - Batch 120/249 - Min Loss:24.95 - Loss:24.956763425100256\n",
      " Iter:0 - Alpha:0.05 - Batch 121/249 - Min Loss:24.86 - Loss:24.869365133151632\n",
      " Iter:0 - Alpha:0.05 - Batch 122/249 - Min Loss:24.77 - Loss:24.772359335267513\n",
      " Iter:0 - Alpha:0.05 - Batch 123/249 - Min Loss:24.67 - Loss:24.675246909839785\n",
      " Iter:0 - Alpha:0.05 - Batch 124/249 - Min Loss:24.59 - Loss:24.5904820724892\n",
      " Iter:0 - Alpha:0.05 - Batch 125/249 - Min Loss:24.51 - Loss:24.51429777515585\n",
      " Iter:0 - Alpha:0.05 - Batch 126/249 - Min Loss:24.44 - Loss:24.449235533650484\n",
      " Iter:0 - Alpha:0.05 - Batch 127/249 - Min Loss:24.36 - Loss:24.367715103209438\n",
      " Iter:0 - Alpha:0.05 - Batch 128/249 - Min Loss:24.26 - Loss:24.260267706792632\n",
      " Iter:0 - Alpha:0.05 - Batch 129/249 - Min Loss:24.16 - Loss:24.167735135560868\n",
      " Iter:0 - Alpha:0.05 - Batch 130/249 - Min Loss:24.09 - Loss:24.096947000483585\n",
      " Iter:0 - Alpha:0.05 - Batch 131/249 - Min Loss:24.00 - Loss:24.005457283760837\n",
      " Iter:0 - Alpha:0.05 - Batch 132/249 - Min Loss:23.90 - Loss:23.909772256462734\n",
      " Iter:0 - Alpha:0.05 - Batch 133/249 - Min Loss:23.82 - Loss:23.824384997876336\n",
      " Iter:0 - Alpha:0.05 - Batch 134/249 - Min Loss:23.73 - Loss:23.732968069915184\n",
      " Iter:0 - Alpha:0.05 - Batch 135/249 - Min Loss:23.68 - Loss:23.680155015926672\n",
      " Iter:0 - Alpha:0.05 - Batch 136/249 - Min Loss:23.65 - Loss:23.659151342220586\n",
      " Iter:0 - Alpha:0.05 - Batch 137/249 - Min Loss:23.59 - Loss:23.592139520674525\n",
      " Iter:0 - Alpha:0.05 - Batch 138/249 - Min Loss:23.52 - Loss:23.520573307081218\n",
      " Iter:0 - Alpha:0.05 - Batch 140/249 - Min Loss:23.50 - Loss:23.52328868575408\n",
      " Iter:0 - Alpha:0.05 - Batch 141/249 - Min Loss:23.45 - Loss:23.45707621234342\n",
      " Iter:0 - Alpha:0.05 - Batch 142/249 - Min Loss:23.37 - Loss:23.37199703427727\n",
      " Iter:0 - Alpha:0.05 - Batch 143/249 - Min Loss:23.28 - Loss:23.28981926078608\n",
      " Iter:0 - Alpha:0.05 - Batch 144/249 - Min Loss:23.20 - Loss:23.20273241054819\n",
      " Iter:0 - Alpha:0.05 - Batch 145/249 - Min Loss:23.12 - Loss:23.129403558475648\n",
      " Iter:0 - Alpha:0.05 - Batch 146/249 - Min Loss:23.09 - Loss:23.097979419016387\n",
      " Iter:0 - Alpha:0.05 - Batch 147/249 - Min Loss:23.05 - Loss:23.05119329819381\n",
      " Iter:0 - Alpha:0.05 - Batch 148/249 - Min Loss:23.01 - Loss:23.01714204129688\n",
      " Iter:0 - Alpha:0.05 - Batch 149/249 - Min Loss:22.94 - Loss:22.943280863548598\n",
      " Iter:0 - Alpha:0.05 - Batch 150/249 - Min Loss:22.87 - Loss:22.874071091357738\n",
      " Iter:0 - Alpha:0.05 - Batch 151/249 - Min Loss:22.82 - Loss:22.824358121803602 - he there theren theren theren theren theren theren theren theren there\n",
      " Iter:0 - Alpha:0.05 - Batch 152/249 - Min Loss:22.75 - Loss:22.753066638496065\n",
      " Iter:0 - Alpha:0.05 - Batch 153/249 - Min Loss:22.66 - Loss:22.66883628925898\n",
      " Iter:0 - Alpha:0.05 - Batch 154/249 - Min Loss:22.60 - Loss:22.60513168195469\n",
      " Iter:0 - Alpha:0.05 - Batch 155/249 - Min Loss:22.56 - Loss:22.5670276745489\n",
      " Iter:0 - Alpha:0.05 - Batch 156/249 - Min Loss:22.54 - Loss:22.544989234452604\n",
      " Iter:0 - Alpha:0.05 - Batch 157/249 - Min Loss:22.49 - Loss:22.499364086457536\n",
      " Iter:0 - Alpha:0.05 - Batch 158/249 - Min Loss:22.44 - Loss:22.44202686260243\n",
      " Iter:0 - Alpha:0.05 - Batch 159/249 - Min Loss:22.40 - Loss:22.404962804966665\n",
      " Iter:0 - Alpha:0.05 - Batch 160/249 - Min Loss:22.34 - Loss:22.345902598191266\n",
      " Iter:0 - Alpha:0.05 - Batch 161/249 - Min Loss:22.28 - Loss:22.280248727477936\n",
      " Iter:0 - Alpha:0.05 - Batch 162/249 - Min Loss:22.22 - Loss:22.224996608030448\n",
      " Iter:0 - Alpha:0.05 - Batch 163/249 - Min Loss:22.16 - Loss:22.16759828923908\n",
      " Iter:0 - Alpha:0.05 - Batch 164/249 - Min Loss:22.12 - Loss:22.121561479876295\n",
      " Iter:0 - Alpha:0.05 - Batch 165/249 - Min Loss:22.08 - Loss:22.08184110343021\n",
      " Iter:0 - Alpha:0.05 - Batch 166/249 - Min Loss:22.02 - Loss:22.029436810087343\n",
      " Iter:0 - Alpha:0.05 - Batch 167/249 - Min Loss:22.00 - Loss:22.001805861786863\n",
      " Iter:0 - Alpha:0.05 - Batch 168/249 - Min Loss:21.96 - Loss:21.968875516706216\n",
      " Iter:0 - Alpha:0.05 - Batch 169/249 - Min Loss:21.91 - Loss:21.91193078232508\n",
      " Iter:0 - Alpha:0.05 - Batch 170/249 - Min Loss:21.86 - Loss:21.86468211456286\n",
      " Iter:0 - Alpha:0.05 - Batch 171/249 - Min Loss:21.82 - Loss:21.82604471209948\n",
      " Iter:0 - Alpha:0.05 - Batch 172/249 - Min Loss:21.79 - Loss:21.792075601991424\n",
      " Iter:0 - Alpha:0.05 - Batch 173/249 - Min Loss:21.75 - Loss:21.752289439198243\n",
      " Iter:0 - Alpha:0.05 - Batch 174/249 - Min Loss:21.71 - Loss:21.718318702198452\n",
      " Iter:0 - Alpha:0.05 - Batch 175/249 - Min Loss:21.69 - Loss:21.699372887522717\n",
      " Iter:0 - Alpha:0.05 - Batch 176/249 - Min Loss:21.65 - Loss:21.659417121282953\n",
      " Iter:0 - Alpha:0.05 - Batch 177/249 - Min Loss:21.61 - Loss:21.61137418513008\n",
      " Iter:0 - Alpha:0.05 - Batch 178/249 - Min Loss:21.56 - Loss:21.565651401093366\n",
      " Iter:0 - Alpha:0.05 - Batch 179/249 - Min Loss:21.54 - Loss:21.54316011293041\n",
      " Iter:0 - Alpha:0.05 - Batch 180/249 - Min Loss:21.49 - Loss:21.498057513193938\n",
      " Iter:0 - Alpha:0.05 - Batch 181/249 - Min Loss:21.44 - Loss:21.449478290289104\n",
      " Iter:0 - Alpha:0.05 - Batch 182/249 - Min Loss:21.39 - Loss:21.399640191440664\n",
      " Iter:0 - Alpha:0.05 - Batch 183/249 - Min Loss:21.34 - Loss:21.3469549400649\n",
      " Iter:0 - Alpha:0.05 - Batch 184/249 - Min Loss:21.30 - Loss:21.305236092340316\n",
      " Iter:0 - Alpha:0.05 - Batch 185/249 - Min Loss:21.25 - Loss:21.259005143665878\n",
      " Iter:0 - Alpha:0.05 - Batch 186/249 - Min Loss:21.22 - Loss:21.222836871880922\n",
      " Iter:0 - Alpha:0.05 - Batch 188/249 - Min Loss:21.19 - Loss:21.200400396642966\n",
      " Iter:0 - Alpha:0.05 - Batch 189/249 - Min Loss:21.16 - Loss:21.165214326033766\n",
      " Iter:0 - Alpha:0.05 - Batch 190/249 - Min Loss:21.13 - Loss:21.138628603486957\n",
      " Iter:0 - Alpha:0.05 - Batch 191/249 - Min Loss:21.11 - Loss:21.11101982206068\n",
      " Iter:0 - Alpha:0.05 - Batch 192/249 - Min Loss:21.07 - Loss:21.079357985241394\n",
      " Iter:0 - Alpha:0.05 - Batch 193/249 - Min Loss:21.01 - Loss:21.01382443198556\n",
      " Iter:0 - Alpha:0.05 - Batch 194/249 - Min Loss:20.99 - Loss:20.991610437214224\n",
      " Iter:0 - Alpha:0.05 - Batch 195/249 - Min Loss:20.97 - Loss:20.979688322539204\n",
      " Iter:0 - Alpha:0.05 - Batch 196/249 - Min Loss:20.93 - Loss:20.93325912594995\n",
      " Iter:0 - Alpha:0.05 - Batch 197/249 - Min Loss:20.90 - Loss:20.9002380460984\n",
      " Iter:0 - Alpha:0.05 - Batch 198/249 - Min Loss:20.87 - Loss:20.871587247129813\n",
      " Iter:0 - Alpha:0.05 - Batch 199/249 - Min Loss:20.82 - Loss:20.82876650904072\n",
      " Iter:0 - Alpha:0.05 - Batch 200/249 - Min Loss:20.77 - Loss:20.77956235086405\n",
      " Iter:0 - Alpha:0.05 - Batch 201/249 - Min Loss:20.74 - Loss:20.74969375377584 - here there, therer,  There werer,  There werer,  There werer,  There w\n",
      " Iter:0 - Alpha:0.05 - Batch 202/249 - Min Loss:20.72 - Loss:20.723302030026787\n",
      " Iter:0 - Alpha:0.05 - Batch 203/249 - Min Loss:20.68 - Loss:20.68609889326973\n",
      " Iter:0 - Alpha:0.05 - Batch 204/249 - Min Loss:20.64 - Loss:20.648981683078254\n",
      " Iter:0 - Alpha:0.05 - Batch 205/249 - Min Loss:20.61 - Loss:20.61603403192446\n",
      " Iter:0 - Alpha:0.05 - Batch 206/249 - Min Loss:20.57 - Loss:20.572906871165973\n",
      " Iter:0 - Alpha:0.05 - Batch 207/249 - Min Loss:20.53 - Loss:20.534994777317177\n",
      " Iter:0 - Alpha:0.05 - Batch 208/249 - Min Loss:20.49 - Loss:20.496871570764828\n",
      " Iter:0 - Alpha:0.05 - Batch 209/249 - Min Loss:20.44 - Loss:20.448959011020815\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Iter:0 - Alpha:0.05 - Batch 210/249 - Min Loss:20.40 - Loss:20.402615723799737\n",
      " Iter:0 - Alpha:0.05 - Batch 211/249 - Min Loss:20.34 - Loss:20.34783610156164\n",
      " Iter:0 - Alpha:0.05 - Batch 212/249 - Min Loss:20.30 - Loss:20.30214444560546\n",
      " Iter:0 - Alpha:0.05 - Batch 213/249 - Min Loss:20.26 - Loss:20.26682238038663\n",
      " Iter:0 - Alpha:0.05 - Batch 214/249 - Min Loss:20.23 - Loss:20.232305846644827\n",
      " Iter:0 - Alpha:0.05 - Batch 215/249 - Min Loss:20.18 - Loss:20.187011667752035\n",
      " Iter:0 - Alpha:0.05 - Batch 216/249 - Min Loss:20.14 - Loss:20.146346829590534\n",
      " Iter:0 - Alpha:0.05 - Batch 217/249 - Min Loss:20.11 - Loss:20.11928420597005\n",
      " Iter:0 - Alpha:0.05 - Batch 218/249 - Min Loss:20.07 - Loss:20.074548484059285\n",
      " Iter:0 - Alpha:0.05 - Batch 219/249 - Min Loss:20.01 - Loss:20.01951378900323\n",
      " Iter:0 - Alpha:0.05 - Batch 220/249 - Min Loss:19.99 - Loss:19.991943801041593\n",
      " Iter:0 - Alpha:0.05 - Batch 221/249 - Min Loss:19.98 - Loss:19.982170326866118\n",
      " Iter:0 - Alpha:0.05 - Batch 222/249 - Min Loss:19.94 - Loss:19.94885360405926\n",
      " Iter:0 - Alpha:0.05 - Batch 223/249 - Min Loss:19.91 - Loss:19.915248141610217\n",
      " Iter:0 - Alpha:0.05 - Batch 224/249 - Min Loss:19.88 - Loss:19.884869959965503\n",
      " Iter:0 - Alpha:0.05 - Batch 225/249 - Min Loss:19.85 - Loss:19.850516245507446\n",
      " Iter:0 - Alpha:0.05 - Batch 226/249 - Min Loss:19.81 - Loss:19.811189172110925\n",
      " Iter:0 - Alpha:0.05 - Batch 227/249 - Min Loss:19.78 - Loss:19.78001118958957\n",
      " Iter:0 - Alpha:0.05 - Batch 228/249 - Min Loss:19.76 - Loss:19.762540484580576\n",
      " Iter:0 - Alpha:0.05 - Batch 229/249 - Min Loss:19.73 - Loss:19.73094769346799\n",
      " Iter:0 - Alpha:0.05 - Batch 230/249 - Min Loss:19.70 - Loss:19.701195752983963\n",
      " Iter:0 - Alpha:0.05 - Batch 231/249 - Min Loss:19.68 - Loss:19.682942558215604\n",
      " Iter:0 - Alpha:0.05 - Batch 232/249 - Min Loss:19.64 - Loss:19.649809743805193\n",
      " Iter:0 - Alpha:0.05 - Batch 233/249 - Min Loss:19.62 - Loss:19.629631641733063\n",
      " Iter:0 - Alpha:0.05 - Batch 234/249 - Min Loss:19.59 - Loss:19.59578085458759\n",
      " Iter:0 - Alpha:0.05 - Batch 235/249 - Min Loss:19.55 - Loss:19.55953347108004\n",
      " Iter:0 - Alpha:0.05 - Batch 236/249 - Min Loss:19.51 - Loss:19.519915960896892\n",
      " Iter:0 - Alpha:0.05 - Batch 237/249 - Min Loss:19.49 - Loss:19.49235830307719\n",
      " Iter:0 - Alpha:0.05 - Batch 238/249 - Min Loss:19.46 - Loss:19.467759647013942\n",
      " Iter:0 - Alpha:0.05 - Batch 239/249 - Min Loss:19.44 - Loss:19.44943876309836\n",
      " Iter:0 - Alpha:0.05 - Batch 240/249 - Min Loss:19.43 - Loss:19.432961881753855\n",
      " Iter:0 - Alpha:0.05 - Batch 241/249 - Min Loss:19.39 - Loss:19.394082888392045\n",
      " Iter:0 - Alpha:0.05 - Batch 242/249 - Min Loss:19.36 - Loss:19.362036660332915\n",
      " Iter:0 - Alpha:0.05 - Batch 244/249 - Min Loss:19.33 - Loss:19.333419773106265\n",
      " Iter:0 - Alpha:0.05 - Batch 245/249 - Min Loss:19.31 - Loss:19.3130408635012\n",
      " Iter:0 - Alpha:0.05 - Batch 246/249 - Min Loss:19.29 - Loss:19.290180303700954\n",
      " Iter:0 - Alpha:0.05 - Batch 247/249 - Min Loss:19.26 - Loss:19.263727929055346\n",
      " Iter:0 - Alpha:0.05 - Batch 248/249 - Min Loss:19.23 - Loss:19.23393261963294\n",
      " Iter:0 - Alpha:0.05 - Batch 249/249 - Min Loss:19.20 - Loss:19.207720008016533\n",
      " Iter:1 - Alpha:0.049 - Batch 1/249 - Min Loss:12.99 - Loss:12.994044957624068 - her ther ther ther ther ther ther ther ther ther ther ther ther ther t\n",
      " Iter:1 - Alpha:0.049 - Batch 3/249 - Min Loss:12.83 - Loss:12.927520398789467\n",
      " Iter:1 - Alpha:0.049 - Batch 4/249 - Min Loss:12.78 - Loss:12.788578203701627\n",
      " Iter:2 - Alpha:0.049 - Batch 34/249 - Min Loss:12.71 - Loss:12.741029729771299- hend thend thend thend thend thend thend thend thend thend thend thend e\n",
      " Iter:2 - Alpha:0.049 - Batch 35/249 - Min Loss:12.69 - Loss:12.691724167072106\n",
      " Iter:2 - Alpha:0.049 - Batch 97/249 - Min Loss:12.67 - Loss:12.685913860800634- h ath a se ath at a a se ath at a a se ath at a a se ath at a a se ath\n",
      " Iter:2 - Alpha:0.049 - Batch 98/249 - Min Loss:12.66 - Loss:12.66982032083269\n",
      " Iter:2 - Alpha:0.049 - Batch 99/249 - Min Loss:12.66 - Loss:12.668297980140554\n",
      " Iter:2 - Alpha:0.049 - Batch 100/249 - Min Loss:12.66 - Loss:12.660100313499129\n",
      " Iter:2 - Alpha:0.049 - Batch 101/249 - Min Loss:12.65 - Loss:12.652897624856559 - he the the the the the the the the the the the the the the the the the\n",
      " Iter:2 - Alpha:0.049 - Batch 192/249 - Min Loss:12.65 - Loss:12.664095376549849 - herererererend thererend witherend witherend witherend witherend withe\n",
      " Iter:2 - Alpha:0.049 - Batch 195/249 - Min Loss:12.64 - Loss:12.651577899518738\n",
      " Iter:2 - Alpha:0.049 - Batch 198/249 - Min Loss:12.64 - Loss:12.644796689873084\n",
      " Iter:2 - Alpha:0.049 - Batch 199/249 - Min Loss:12.64 - Loss:12.64092618847861\n",
      " Iter:2 - Alpha:0.049 - Batch 200/249 - Min Loss:12.63 - Loss:12.634273243618996\n",
      " Iter:2 - Alpha:0.049 - Batch 201/249 - Min Loss:12.63 - Loss:12.630288887702568 - herer, will will will will will will will will will will will will wil\n",
      " Iter:2 - Alpha:0.049 - Batch 202/249 - Min Loss:12.62 - Loss:12.629424820643905\n",
      " Iter:2 - Alpha:0.049 - Batch 203/249 - Min Loss:12.62 - Loss:12.629400547767451\n",
      " Iter:2 - Alpha:0.049 - Batch 204/249 - Min Loss:12.62 - Loss:12.62422529762664\n",
      " Iter:2 - Alpha:0.049 - Batch 205/249 - Min Loss:12.62 - Loss:12.623601215083752\n",
      " Iter:2 - Alpha:0.049 - Batch 206/249 - Min Loss:12.61 - Loss:12.617372329458696\n",
      " Iter:2 - Alpha:0.049 - Batch 207/249 - Min Loss:12.60 - Loss:12.60907664783462\n",
      " Iter:2 - Alpha:0.049 - Batch 208/249 - Min Loss:12.60 - Loss:12.60285613378434\n",
      " Iter:2 - Alpha:0.049 - Batch 209/249 - Min Loss:12.59 - Loss:12.592936114652542\n",
      " Iter:2 - Alpha:0.049 - Batch 210/249 - Min Loss:12.58 - Loss:12.583620588443274\n",
      " Iter:2 - Alpha:0.049 - Batch 211/249 - Min Loss:12.56 - Loss:12.566064370773864\n",
      " Iter:2 - Alpha:0.049 - Batch 212/249 - Min Loss:12.55 - Loss:12.557763929523036\n",
      " Iter:2 - Alpha:0.049 - Batch 213/249 - Min Loss:12.55 - Loss:12.550883096676937\n",
      " Iter:2 - Alpha:0.049 - Batch 214/249 - Min Loss:12.54 - Loss:12.546927246163648\n",
      " Iter:2 - Alpha:0.049 - Batch 215/249 - Min Loss:12.53 - Loss:12.535996458591967\n",
      " Iter:2 - Alpha:0.049 - Batch 217/249 - Min Loss:12.52 - Loss:12.529866061318812\n",
      " Iter:2 - Alpha:0.049 - Batch 218/249 - Min Loss:12.52 - Loss:12.522480414831444\n",
      " Iter:2 - Alpha:0.049 - Batch 219/249 - Min Loss:12.50 - Loss:12.508120165401111\n",
      " Iter:3 - Alpha:0.048 - Batch 3/249 - Min Loss:12.50 - Loss:12.777980530793729 - hend thend thend thend thend thend thend thend thend thend thend thend\n",
      " Iter:3 - Alpha:0.048 - Batch 4/249 - Min Loss:12.37 - Loss:12.37642986540984\n",
      " Iter:3 - Alpha:0.048 - Batch 25/249 - Min Loss:12.21 - Loss:12.291669342171226\n",
      " Iter:3 - Alpha:0.048 - Batch 31/249 - Min Loss:12.19 - Loss:12.195032978118313\n",
      " Iter:3 - Alpha:0.048 - Batch 33/249 - Min Loss:12.15 - Loss:12.174181792065183\n",
      " Iter:3 - Alpha:0.048 - Batch 34/249 - Min Loss:12.15 - Loss:12.154080974906131\n",
      " Iter:3 - Alpha:0.048 - Batch 107/249 - Min Loss:12.11 - Loss:12.129539645981182 - h the th th th th th th th th th th th th th th th th th th th th th t\n",
      " Iter:3 - Alpha:0.048 - Batch 130/249 - Min Loss:12.11 - Loss:12.119167302246526\n",
      " Iter:3 - Alpha:0.048 - Batch 131/249 - Min Loss:12.09 - Loss:12.099332925418903\n",
      " Iter:3 - Alpha:0.048 - Batch 133/249 - Min Loss:12.08 - Loss:12.089422893574449\n",
      " Iter:3 - Alpha:0.048 - Batch 135/249 - Min Loss:12.08 - Loss:12.087322527590757\n",
      " Iter:3 - Alpha:0.048 - Batch 137/249 - Min Loss:12.07 - Loss:12.075391143059469\n",
      " Iter:3 - Alpha:0.048 - Batch 152/249 - Min Loss:12.06 - Loss:12.066334608417487 - hererers and thererers and thererers and thererers and thererers and t\n",
      " Iter:3 - Alpha:0.048 - Batch 153/249 - Min Loss:12.05 - Loss:12.05294807605315\n",
      " Iter:3 - Alpha:0.048 - Batch 206/249 - Min Loss:12.04 - Loss:12.048406297479904 - hat will will wit will wit will wit will wit will wit will wit will wi\n",
      " Iter:3 - Alpha:0.048 - Batch 207/249 - Min Loss:12.04 - Loss:12.041805699268894\n",
      " Iter:3 - Alpha:0.048 - Batch 208/249 - Min Loss:12.03 - Loss:12.035897891052194\n",
      " Iter:3 - Alpha:0.048 - Batch 209/249 - Min Loss:12.02 - Loss:12.02562323116531\n",
      " Iter:3 - Alpha:0.048 - Batch 210/249 - Min Loss:12.01 - Loss:12.016863460255149\n",
      " Iter:3 - Alpha:0.048 - Batch 211/249 - Min Loss:11.99 - Loss:11.998244175558497\n",
      " Iter:3 - Alpha:0.048 - Batch 212/249 - Min Loss:11.98 - Loss:11.989746889295823\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Iter:3 - Alpha:0.048 - Batch 213/249 - Min Loss:11.98 - Loss:11.984975117559008\n",
      " Iter:3 - Alpha:0.048 - Batch 214/249 - Min Loss:11.98 - Loss:11.98268269133983\n",
      " Iter:3 - Alpha:0.048 - Batch 215/249 - Min Loss:11.97 - Loss:11.971769072520187\n",
      " Iter:3 - Alpha:0.048 - Batch 217/249 - Min Loss:11.96 - Loss:11.964181720342618\n",
      " Iter:3 - Alpha:0.048 - Batch 218/249 - Min Loss:11.95 - Loss:11.957087928976922\n",
      " Iter:3 - Alpha:0.048 - Batch 223/249 - Min Loss:11.94 - Loss:11.946467339765988\n",
      " Iter:3 - Alpha:0.048 - Batch 224/249 - Min Loss:11.94 - Loss:11.94044622734134\n",
      " Iter:3 - Alpha:0.048 - Batch 225/249 - Min Loss:11.93 - Loss:11.936476877724955\n",
      " Iter:3 - Alpha:0.048 - Batch 236/249 - Min Loss:11.93 - Loss:11.934189138386955\n",
      " Iter:4 - Alpha:0.048 - Batch 2/249 - Min Loss:11.93 - Loss:12.161941588790109 - hend theseres, and theseres, and theseres, and theseres, and theseres,\n",
      " Iter:4 - Alpha:0.048 - Batch 3/249 - Min Loss:11.84 - Loss:11.84283678209681\n",
      " Iter:4 - Alpha:0.048 - Batch 4/249 - Min Loss:11.76 - Loss:11.76881597797434\n",
      " Iter:4 - Alpha:0.048 - Batch 98/249 - Min Loss:11.70 - Loss:11.707363922392586 - here there there there there there there there there there there there\n",
      " Iter:4 - Alpha:0.048 - Batch 99/249 - Min Loss:11.70 - Loss:11.704385086329596\n",
      " Iter:4 - Alpha:0.048 - Batch 100/249 - Min Loss:11.69 - Loss:11.699436491948493\n",
      " Iter:4 - Alpha:0.048 - Batch 101/249 - Min Loss:11.69 - Loss:11.69252763454726 - I And the the the the the the the the the the the the the the the the \n",
      " Iter:4 - Alpha:0.048 - Batch 102/249 - Min Loss:11.67 - Loss:11.674517622686174\n",
      " Iter:4 - Alpha:0.048 - Batch 103/249 - Min Loss:11.66 - Loss:11.66744443615261\n",
      " Iter:4 - Alpha:0.048 - Batch 104/249 - Min Loss:11.66 - Loss:11.66489747827397\n",
      " Iter:4 - Alpha:0.048 - Batch 105/249 - Min Loss:11.66 - Loss:11.660915984694267\n",
      " Iter:4 - Alpha:0.048 - Batch 106/249 - Min Loss:11.64 - Loss:11.645987900499152\n",
      " Iter:4 - Alpha:0.048 - Batch 107/249 - Min Loss:11.64 - Loss:11.645861120711885\n",
      " Iter:4 - Alpha:0.048 - Batch 126/249 - Min Loss:11.63 - Loss:11.646239997278059\n",
      " Iter:4 - Alpha:0.048 - Batch 127/249 - Min Loss:11.63 - Loss:11.635344407216966\n",
      " Iter:4 - Alpha:0.048 - Batch 128/249 - Min Loss:11.62 - Loss:11.628719334978047\n",
      " Iter:4 - Alpha:0.048 - Batch 129/249 - Min Loss:11.61 - Loss:11.618414003119716\n",
      " Iter:4 - Alpha:0.048 - Batch 130/249 - Min Loss:11.61 - Loss:11.612178572839937\n",
      " Iter:4 - Alpha:0.048 - Batch 131/249 - Min Loss:11.59 - Loss:11.594866297245188\n",
      " Iter:4 - Alpha:0.048 - Batch 133/249 - Min Loss:11.57 - Loss:11.585514464773865\n",
      " Iter:4 - Alpha:0.048 - Batch 135/249 - Min Loss:11.57 - Loss:11.579140252511447\n",
      " Iter:4 - Alpha:0.048 - Batch 136/249 - Min Loss:11.56 - Loss:11.564437698602418\n",
      " Iter:4 - Alpha:0.048 - Batch 137/249 - Min Loss:11.56 - Loss:11.563932130864714\n",
      " Iter:4 - Alpha:0.048 - Batch 152/249 - Min Loss:11.54 - Loss:11.546109056340093 - hererers and thererers and thererers and thererers and thererers and t\n",
      " Iter:4 - Alpha:0.048 - Batch 153/249 - Min Loss:11.53 - Loss:11.534791078769674\n",
      " Iter:4 - Alpha:0.048 - Batch 206/249 - Min Loss:11.52 - Loss:11.530754215137614 - hat willed willed willed willed willed willed willed willed willed wil\n",
      " Iter:4 - Alpha:0.048 - Batch 207/249 - Min Loss:11.52 - Loss:11.527176960458688\n",
      " Iter:4 - Alpha:0.048 - Batch 208/249 - Min Loss:11.52 - Loss:11.521527051847707\n",
      " Iter:4 - Alpha:0.048 - Batch 209/249 - Min Loss:11.51 - Loss:11.511607033071337\n",
      " Iter:4 - Alpha:0.048 - Batch 210/249 - Min Loss:11.50 - Loss:11.503734818834848\n",
      " Iter:4 - Alpha:0.048 - Batch 211/249 - Min Loss:11.48 - Loss:11.484912623196406\n",
      " Iter:4 - Alpha:0.048 - Batch 212/249 - Min Loss:11.47 - Loss:11.4774640549616\n",
      " Iter:4 - Alpha:0.048 - Batch 213/249 - Min Loss:11.47 - Loss:11.473259384019551\n",
      " Iter:4 - Alpha:0.048 - Batch 214/249 - Min Loss:11.47 - Loss:11.471855674676183\n",
      " Iter:4 - Alpha:0.048 - Batch 215/249 - Min Loss:11.46 - Loss:11.460673173909365\n",
      " Iter:4 - Alpha:0.048 - Batch 217/249 - Min Loss:11.45 - Loss:11.454909851334975\n",
      " Iter:4 - Alpha:0.048 - Batch 218/249 - Min Loss:11.44 - Loss:11.448241389168558\n",
      " Iter:4 - Alpha:0.048 - Batch 221/249 - Min Loss:11.43 - Loss:11.440556670462819\n",
      " Iter:4 - Alpha:0.048 - Batch 222/249 - Min Loss:11.43 - Loss:11.434316958185645\n",
      " Iter:4 - Alpha:0.048 - Batch 223/249 - Min Loss:11.42 - Loss:11.429349585222534\n",
      " Iter:4 - Alpha:0.048 - Batch 224/249 - Min Loss:11.42 - Loss:11.423759216424909\n",
      " Iter:4 - Alpha:0.048 - Batch 225/249 - Min Loss:11.41 - Loss:11.418550323714344\n",
      " Iter:4 - Alpha:0.048 - Batch 226/249 - Min Loss:11.41 - Loss:11.4150978407145\n",
      " Iter:4 - Alpha:0.048 - Batch 227/249 - Min Loss:11.41 - Loss:11.411387426997525\n",
      " Iter:5 - Alpha:0.047 - Batch 2/249 - Min Loss:11.40 - Loss:11.499194934901871 - hest and seat theser theser theser theser theser theser theser theser \n",
      " Iter:5 - Alpha:0.047 - Batch 152/249 - Min Loss:11.10 - Loss:11.106587733747062 - here, willy there, willy there, willy there, willy there, willy there,\n",
      " Iter:5 - Alpha:0.047 - Batch 153/249 - Min Loss:11.09 - Loss:11.097248093616352\n",
      " Iter:5 - Alpha:0.047 - Batch 203/249 - Min Loss:11.08 - Loss:11.092249741362599 - hat willed willed willed willed willed willed willed willed willed wil\n",
      " Iter:5 - Alpha:0.047 - Batch 206/249 - Min Loss:11.08 - Loss:11.089052480469688\n",
      " Iter:5 - Alpha:0.047 - Batch 207/249 - Min Loss:11.08 - Loss:11.087192108372516\n",
      " Iter:5 - Alpha:0.047 - Batch 208/249 - Min Loss:11.08 - Loss:11.081709950335155\n",
      " Iter:5 - Alpha:0.047 - Batch 209/249 - Min Loss:11.07 - Loss:11.073350850810634\n",
      " Iter:5 - Alpha:0.047 - Batch 210/249 - Min Loss:11.06 - Loss:11.066723468478367\n",
      " Iter:5 - Alpha:0.047 - Batch 211/249 - Min Loss:11.04 - Loss:11.048332083881418\n",
      " Iter:5 - Alpha:0.047 - Batch 212/249 - Min Loss:11.04 - Loss:11.042046676461805\n",
      " Iter:5 - Alpha:0.047 - Batch 213/249 - Min Loss:11.03 - Loss:11.037004830741298\n",
      " Iter:5 - Alpha:0.047 - Batch 214/249 - Min Loss:11.03 - Loss:11.035599652078789\n",
      " Iter:5 - Alpha:0.047 - Batch 215/249 - Min Loss:11.02 - Loss:11.024439691734091\n",
      " Iter:5 - Alpha:0.047 - Batch 217/249 - Min Loss:11.01 - Loss:11.019232000003262\n",
      " Iter:5 - Alpha:0.047 - Batch 218/249 - Min Loss:11.01 - Loss:11.011681852807241\n",
      " Iter:5 - Alpha:0.047 - Batch 219/249 - Min Loss:10.99 - Loss:10.997233131969255\n",
      " Iter:5 - Alpha:0.047 - Batch 221/249 - Min Loss:10.99 - Loss:10.998417642205466\n",
      " Iter:5 - Alpha:0.047 - Batch 222/249 - Min Loss:10.99 - Loss:10.992335682204413\n",
      " Iter:5 - Alpha:0.047 - Batch 223/249 - Min Loss:10.98 - Loss:10.987884984869748\n",
      " Iter:5 - Alpha:0.047 - Batch 224/249 - Min Loss:10.98 - Loss:10.9837822886027\n",
      " Iter:5 - Alpha:0.047 - Batch 225/249 - Min Loss:10.97 - Loss:10.979043115458625\n",
      " Iter:5 - Alpha:0.047 - Batch 226/249 - Min Loss:10.97 - Loss:10.976859443823315\n",
      " Iter:5 - Alpha:0.047 - Batch 227/249 - Min Loss:10.97 - Loss:10.972881529271636\n",
      " Iter:6 - Alpha:0.047 - Batch 1/249 - Min Loss:10.97 - Loss:11.16200819239199 - her ther ther ther ther ther ther ther ther ther ther ther ther ther t\n",
      " Iter:6 - Alpha:0.047 - Batch 2/249 - Min Loss:10.94 - Loss:10.944000172239775\n",
      " Iter:7 - Alpha:0.046 - Batch 1/249 - Min Loss:10.61 - Loss:10.726209177236996 - her ther ther ther ther ther ther ther ther ther ther ther ther ther til\n",
      " Iter:7 - Alpha:0.046 - Batch 2/249 - Min Loss:10.57 - Loss:10.571394805412613\n",
      " Iter:8 - Alpha:0.046 - Batch 228/249 - Min Loss:10.42 - Loss:10.422563413655359 - hat will will will will will will will will will will will will will w\n",
      " Iter:8 - Alpha:0.046 - Batch 229/249 - Min Loss:10.42 - Loss:10.421024445385536\n",
      " Iter:8 - Alpha:0.046 - Batch 230/249 - Min Loss:10.41 - Loss:10.41998176626537\n",
      " Iter:8 - Alpha:0.046 - Batch 231/249 - Min Loss:10.41 - Loss:10.417618385700935\n",
      " Iter:8 - Alpha:0.046 - Batch 233/249 - Min Loss:10.41 - Loss:10.418546694268858\n",
      " Iter:8 - Alpha:0.046 - Batch 234/249 - Min Loss:10.41 - Loss:10.411046995623064\n",
      " Iter:8 - Alpha:0.046 - Batch 235/249 - Min Loss:10.40 - Loss:10.403965032279894\n",
      " Iter:8 - Alpha:0.046 - Batch 236/249 - Min Loss:10.39 - Loss:10.395617999628584\n",
      " Iter:8 - Alpha:0.046 - Batch 237/249 - Min Loss:10.38 - Loss:10.387998413717135\n",
      " Iter:8 - Alpha:0.046 - Batch 240/249 - Min Loss:10.38 - Loss:10.392551020274876\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Iter:8 - Alpha:0.046 - Batch 241/249 - Min Loss:10.38 - Loss:10.38366921872776\n",
      " Iter:8 - Alpha:0.046 - Batch 242/249 - Min Loss:10.38 - Loss:10.382002119118807\n",
      " Iter:9 - Alpha:0.045 - Batch 3/249 - Min Loss:10.37 - Loss:10.456776943391993 - heres, and seerer Theat ther ther ther ther ther ther ther ther ther t\n",
      " Iter:9 - Alpha:0.045 - Batch 4/249 - Min Loss:10.35 - Loss:10.35865455757589\n",
      " Iter:9 - Alpha:0.045 - Batch 226/249 - Min Loss:10.33 - Loss:10.336671017005118- hat wille wille ife, wille ife, wille ife, wille ife, wille ife, willeW\n",
      " Iter:9 - Alpha:0.045 - Batch 227/249 - Min Loss:10.33 - Loss:10.330250209152277\n",
      " Iter:9 - Alpha:0.045 - Batch 230/249 - Min Loss:10.32 - Loss:10.330106131007579\n",
      " Iter:9 - Alpha:0.045 - Batch 233/249 - Min Loss:10.32 - Loss:10.331929677786029\n",
      " Iter:9 - Alpha:0.045 - Batch 234/249 - Min Loss:10.32 - Loss:10.32551844199101\n",
      " Iter:9 - Alpha:0.045 - Batch 235/249 - Min Loss:10.32 - Loss:10.320058567110847\n",
      " Iter:9 - Alpha:0.045 - Batch 236/249 - Min Loss:10.31 - Loss:10.312102425130789\n",
      " Iter:9 - Alpha:0.045 - Batch 237/249 - Min Loss:10.30 - Loss:10.304862030810588\n",
      " Iter:9 - Alpha:0.045 - Batch 240/249 - Min Loss:10.30 - Loss:10.310181253979705\n",
      " Iter:9 - Alpha:0.045 - Batch 241/249 - Min Loss:10.30 - Loss:10.301680328740138\n",
      " Iter:9 - Alpha:0.045 - Batch 242/249 - Min Loss:10.30 - Loss:10.301333858335386\n",
      " Iter:10 - Alpha:0.045 - Batch 222/249 - Min Loss:10.29 - Loss:10.300590417725322 - hat willedist ther willest willedist ther willest willedist ther wille\n",
      " Iter:10 - Alpha:0.045 - Batch 223/249 - Min Loss:10.29 - Loss:10.29511305438947\n",
      " Iter:10 - Alpha:0.045 - Batch 224/249 - Min Loss:10.28 - Loss:10.289420322263853\n",
      " Iter:10 - Alpha:0.045 - Batch 225/249 - Min Loss:10.28 - Loss:10.282429477603555\n",
      " Iter:10 - Alpha:0.045 - Batch 226/249 - Min Loss:10.27 - Loss:10.278290074540903\n",
      " Iter:10 - Alpha:0.045 - Batch 227/249 - Min Loss:10.27 - Loss:10.273456028352502\n",
      " Iter:10 - Alpha:0.045 - Batch 228/249 - Min Loss:10.27 - Loss:10.272403314921364\n",
      " Iter:10 - Alpha:0.045 - Batch 233/249 - Min Loss:10.27 - Loss:10.275594979916452\n",
      " Iter:10 - Alpha:0.045 - Batch 234/249 - Min Loss:10.26 - Loss:10.269256860825967\n",
      " Iter:10 - Alpha:0.045 - Batch 235/249 - Min Loss:10.26 - Loss:10.26349655238933\n",
      " Iter:10 - Alpha:0.045 - Batch 236/249 - Min Loss:10.25 - Loss:10.256070539315498\n",
      " Iter:10 - Alpha:0.045 - Batch 237/249 - Min Loss:10.25 - Loss:10.250836304215799\n",
      " Iter:10 - Alpha:0.045 - Batch 242/249 - Min Loss:10.24 - Loss:10.252542477870328\n",
      " Iter:12 - Alpha:0.044 - Batch 3/249 - Min Loss:10.24 - Loss:10.287042687168105 - hat seen ther That seen ther That seen ther That seen ther That seen til\n",
      " Iter:12 - Alpha:0.044 - Batch 4/249 - Min Loss:10.05 - Loss:10.05177432355233\n",
      " Iter:13 - Alpha:0.043 - Batch 3/249 - Min Loss:10.04 - Loss:10.199603788406494- hat sees ather That sees ather That sees ather That sees ather That sewil\n",
      " Iter:13 - Alpha:0.043 - Batch 4/249 - Min Loss:10.03 - Loss:10.032310069048059\n",
      " Iter:99 - Alpha:0.018 - Batch 249/249 - Min Loss:10.02 - Loss:12.706136176551457 - hich will dist and eiser, oullee Let died, will.  LADINGLLAUS: Thich w"
     ]
    }
   ],
   "source": [
    "train(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "And will of will o will lo will  of in the with ir wiove lould will lou did, will oull will lo will po with to lord;\n",
      "And will in to love or will o will lood in the willl o will o will will o will not will lould will o will por trumpos all doul of in to loud will lou in the would bo with to will lood Kor will do will lo doud I do and will loullould bre will not pos of to with the wor will lo dou in of ir, with will lo did will not will look you will look you will look you will lould will o will p\n"
     ]
    }
   ],
   "source": [
    "print(generate_sample(n=500, init_char='\\n'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
